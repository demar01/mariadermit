[{"authors":["maria"],"categories":null,"content":"Hello, and welcome! I\u0026rsquo;m a Postdoctoral Research scientist in Barts Cancer Institute. I work in the Mardakheh Lab where I use multi-omics approaches, to systematically reveal the relationship between RNA localisation and protein expression in mammalian cells. Learn more about my research interests in publications.\nI enjoy using R to optimize my research workflow and I enjoy sharing my experiences in the Tidyverse with others. Keep up with my data exploration using R in posts and teaching in talks. Thanks for reading!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2737be03e93dd16e89f188886e7b4d11","permalink":"/authors/maria/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/maria/","section":"authors","summary":"Hello, and welcome! I\u0026rsquo;m a Postdoctoral Research scientist in Barts Cancer Institute. I work in the Mardakheh Lab where I use multi-omics approaches, to systematically reveal the relationship between RNA localisation and protein expression in mammalian cells. Learn more about my research interests in publications.\nI enjoy using R to optimize my research workflow and I enjoy sharing my experiences in the Tidyverse with others. Keep up with my data exploration using R in posts and teaching in talks.","tags":null,"title":"Maria Dermit","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":["R"],"content":"   Motivation Data loading QC Normalization Variance modelling/feature selection Dimensionality reduction Clustering Interpretation   Motivation Exploration of the Common Framework for Analyzing Single-Cell RNA-seq data.\nThe workflow of the 10X Single-Cell RNA-seq library prep for single-cell expression profiling looks like this:\n\nDr.¬†Eric Chow gives a fantastic overview of Droseq (min 7:08).\nOnce the data is generated the sequencial steps to handle scRNAseq are the following:\nThe central object of the pipeline is SingleCellExperiment, which looks like this:\n\nThe OSCA book is an amazing resource to fully understand the math behind scRNAseq analysis workflow. It has 18 cases studies of scRNAseq.\nThe main steps followed in scRNAseq are:\nData Loading\n QC\n Normalization\n Variance modeling\n Dimensionality reduction\n Clustering\n Interpretation\n   Data loading Here, I am going to use an example of 10X Genomics data the counts matrix and associated metadata (cell barcodes, data path, etc.). This is based on Chapter 27 in OSCA book. The DropletTestFiles package contains files that are the raw output of pipelines like 10X Genomics‚Äô CellRanger software suite, that can be then imported via the DropletUtils package‚Äôs read10xCounts() function.\nlibrary(DropletTestFiles) raw.path \u0026lt;- getTestFile(\u0026quot;tenx-2.1.0-pbmc4k/1.0.0/raw.tar.gz\u0026quot;) out.path \u0026lt;- file.path(tempdir(), \u0026quot;pbmc4k\u0026quot;) untar(raw.path, exdir=out.path) library(DropletUtils) fname \u0026lt;- file.path(out.path, \u0026quot;raw_gene_bc_matrices/GRCh38\u0026quot;) sce.pbmc \u0026lt;- read10xCounts(fname, col.names=TRUE) We need to make sure that we convert the easy interpretable gene symbol to a standard identifier that is guaranteed to be unique and valid (e.g., Ensembl).\nlibrary(scater) rownames(sce.pbmc) \u0026lt;- uniquifyFeatureNames( rowData(sce.pbmc)$ID, rowData(sce.pbmc)$Symbol) library(EnsDb.Hsapiens.v86) location \u0026lt;- mapIds(EnsDb.Hsapiens.v86, keys=rowData(sce.pbmc)$ID, column=\u0026quot;SEQNAME\u0026quot;, keytype=\u0026quot;GENEID\u0026quot;) In addition to this, the scRNAseq package provides convenient access to several publicly available data sets in the form of SingleCellExperiment object.\nlibrary(scRNAseq) out \u0026lt;- listDatasets() dim(out) ## [1] 46 5 head(out) ## DataFrame with 6 rows and 5 columns ## Reference Taxonomy Part Number ## \u0026lt;character\u0026gt; \u0026lt;integer\u0026gt; \u0026lt;character\u0026gt; \u0026lt;integer\u0026gt; ## 1 @aztekin2019identifi.. 8355 tail 13199 ## 2 @bach2017differentia.. 10090 mammary gland 25806 ## 3 @baron2016singlecell 9606 pancreas 8569 ## 4 @baron2016singlecell 10090 pancreas 1886 ## 5 @buettner2015computa.. 10090 embryonic stem cells 288 ## 6 @campbell2017molecular 10090 brain 21086 ## Call ## \u0026lt;character\u0026gt; ## 1 AztekinTailData() ## 2 BachMammaryData() ## 3 BaronPancreasData(\u0026#39;h.. ## 4 BaronPancreasData(\u0026#39;m.. ## 5 BuettnerESCData() ## 6 CampbellBrainData()  QC An unique aspect of droplet-based data is that we have no prior knowledge about whether a particular library (i.e., cell barcode) corresponds to cell-containing or empty droplets. We use the emptyDrops() function to test whether the expression profile for each cell barcode is significantly different from the ambient RNA pool. emptyDrops() assumes that barcodes with low total UMI counts are empty droplets. emptyDrops() uses Monte Carlo simulations to compute p-values for the multinomial sampling transcripts from the ambient pool.\nset.seed(100) #seed forreproducible results, emptyDrops performas a simulation. e.out \u0026lt;- emptyDrops(counts(sce.pbmc)) sce.pbmc \u0026lt;- sce.pbmc[,which(e.out$FDR \u0026lt;= 0.001)] #we exclude dropplets that have low UMI counts  These are the unfiltered, non-empty drops\nunfiltered \u0026lt;- sce.pbmc For each cell, we calculate these QC metrics using the perCellQCMetrics() function from the scater package. The sum column contains the total count for each cell and the detected column contains the number of detected genes. Mitochondrial counts are informative, because small mitochondrial percentages, large spike-in percentages and small library sizes are likely to be stripped nuclei, i.e., they have been so extensively damaged that they have lost all cytoplasmic content, therefore are low quality cells. Here we use a relaxed QC strategy and only remove cells with large mitochondrial proportions, using it as a proxy for cell damage. This reduces the risk of removing cell types with low RNA content, especially in a heterogeneous PBMC population with many different cell types.\nstats \u0026lt;- perCellQCMetrics(sce.pbmc, subsets=list(Mito=which(location==\u0026quot;MT\u0026quot;))) high.mito \u0026lt;- isOutlier(stats$subsets_Mito_percent, type=\u0026quot;higher\u0026quot;) sce.pbmc \u0026lt;- sce.pbmc[,!high.mito] summary(high.mito) ## Mode FALSE TRUE ## logical 3985 315 colData(unfiltered) \u0026lt;- cbind(colData(unfiltered), stats) unfiltered$discard \u0026lt;- high.mito gridExtra::grid.arrange( plotColData(unfiltered, y=\u0026quot;sum\u0026quot;, colour_by=\u0026quot;discard\u0026quot;) + scale_y_log10() + ggtitle(\u0026quot;Total count\u0026quot;), plotColData(unfiltered, y=\u0026quot;detected\u0026quot;, colour_by=\u0026quot;discard\u0026quot;) + scale_y_log10() + ggtitle(\u0026quot;Detected features\u0026quot;), plotColData(unfiltered, y=\u0026quot;subsets_Mito_percent\u0026quot;, colour_by=\u0026quot;discard\u0026quot;) + ggtitle(\u0026quot;Mito percent\u0026quot;), ncol=2 )  Figure 1: Distribution of various QC metrics in the PBMC dataset after cell calling. Each point is a cell and is colored according to whether it was discarded by the mitochondrial filter.  plotColData(unfiltered, x=\u0026quot;sum\u0026quot;, y=\u0026quot;subsets_Mito_percent\u0026quot;, colour_by=\u0026quot;discard\u0026quot;) + scale_x_log10()  Figure 2: Proportion of mitochondrial reads in each cell of the PBMC dataset compared to its total count.   Normalization There are a number of normalization methods. We use a pre-clustering step with quickCluster() where cells in each cluster are normalized separately and the size factors are rescaled to be comparable across clusters. This avoids the assumption that most genes are non-DE across the entire population - only a non-DE majority is required between pairs of clusters, which is a weaker assumption for highly heterogeneous populations. By default, quickCluster() will use an approximate algorithm for PCA based on methods from the irlba package. The approximation relies on stochastic initialization so we need to set the random seed (via set.seed()) for reproducibility. computeSumFactors performs a scaling normalization of single-cell RNA-seq data by deconvolving size factors from cell pools. Once we have computed the size factors, we use the logNormCounts() function from scater to compute normalized expression values for each cell. This is done by dividing the count for each gene/spike-in transcript with the appropriate size factor for that cell. The function also log-transforms the normalized values, creating a new assay called \"logcounts\". (Technically, these are ‚Äúlog-transformed normalized expression values‚Äù, but that‚Äôs too much of a mouthful to fit into the assay name.) These log-values will be the basis of our downstream analyses.\nlibrary(scran) set.seed(1000) clusters \u0026lt;- quickCluster(sce.pbmc) sce.pbmc \u0026lt;- computeSumFactors(sce.pbmc, cluster=clusters) sce.pbmc \u0026lt;- logNormCounts(sce.pbmc) summary(sizeFactors(sce.pbmc)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.00749 0.71207 0.87490 1.00000 1.09900 12.25412 We can plot The ‚Äúlibrary size factor‚Äù for each cell. As for bulk RNAseq, library size normalization is the simplest strategy for performing scaling normalization. However, library size normalization is usually sufficient in many applications where the aim is to identify clusters and the top markers that define each cluster.\nplot(librarySizeFactors(sce.pbmc), sizeFactors(sce.pbmc), pch=16, xlab=\u0026quot;Library size factors\u0026quot;, ylab=\u0026quot;Deconvolution factors\u0026quot;, log=\u0026quot;xy\u0026quot;)  Figure 3: Relationship between the library size factors and the deconvolution size factors in the PBMC dataset.   Variance modelling/feature selection Several methods are available to quantify the variation per gene and to select an appropriate set of highly variable genes (HVGs). UMI counts typically exhibit near-Poisson variation if we only consider technical noise from library preparation and sequencing. This can be used to construct a mean-variance trend in the log-counts with the modelGeneVarByPoisson function. We can then select the top 10% of genes with the highest biological components.\nset.seed(1001) dec.pbmc \u0026lt;- modelGeneVarByPoisson(sce.pbmc) top.pbmc \u0026lt;- getTopHVGs(dec.pbmc, prop=0.1) plot(dec.pbmc$mean, dec.pbmc$total, pch=16, cex=0.5, xlab=\u0026quot;Mean of log-expression\u0026quot;, ylab=\u0026quot;Variance of log-expression\u0026quot;) curfit \u0026lt;- metadata(dec.pbmc) curve(curfit$trend(x), col=\u0026#39;dodgerblue\u0026#39;, add=TRUE, lwd=2)  Figure 4: Per-gene variance as a function of the mean for the log-expression values in the PBMC dataset. Each point represents a gene (black) with the mean-variance trend (blue) fitted to simulated Poisson counts.   Dimensionality reduction Now we want to compare cells based on the values of gene expression. We can use denoisePCA function, that ‚Äúde-noises‚Äù log-expression data by removing principal components corresponding to technical noise.\nset.seed(10000) sce.pbmc \u0026lt;- denoisePCA(sce.pbmc, subset.row=top.pbmc, technical=dec.pbmc) set.seed(100000) sce.pbmc \u0026lt;- runTSNE(sce.pbmc, dimred=\u0026quot;PCA\u0026quot;) set.seed(1000000) sce.pbmc \u0026lt;- runUMAP(sce.pbmc, dimred=\u0026quot;PCA\u0026quot;) We verify how many PCs were retained.\nncol(reducedDim(sce.pbmc, \u0026quot;PCA\u0026quot;)) ## [1] 9  Clustering We can now create nearest-neighbor graphs with the buildSNNGraph function and cluster_walktrap function. cluster_walktrap will assign membership of densely connected subgraphs, also called ‚Äúcommunities‚Äù.\ng \u0026lt;- buildSNNGraph(sce.pbmc, k=10, use.dimred = \u0026#39;PCA\u0026#39;) clust \u0026lt;- igraph::cluster_walktrap(g)$membership colLabels(sce.pbmc) \u0026lt;- factor(clust) table(colLabels(sce.pbmc)) ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 205 508 541 56 374 125 46 432 302 867 47 155 166 61 84 16 plotTSNE(sce.pbmc, colour_by=\u0026quot;label\u0026quot;)  Figure 5: Obligatory \\(t\\)-SNE plot of the PBMC dataset, where each point represents a cell and is colored according to the assigned cluster.   Interpretation To interpret the clustering results, we need to identify the genes that drive separation between clusters. In the most obvious case, the marker genes for each cluster are a priori associated with particular cell types, allowing us to treat the clustering as a proxy for cell type identity. The same principle can be applied to discover more subtle differences between clusters (e.g., changes in activation or differentiation state) based on the behavior of genes in the affected pathways. We perform pairwise t-tests between clusters for each gene using the findMarkers function, which uses a Welch t-test to perform DE testing between clusters and returns a list of DataFrames containing ranked candidate markers for each cluster.\nmarkers \u0026lt;- findMarkers(sce.pbmc, pval.type=\u0026quot;some\u0026quot;, direction=\u0026quot;up\u0026quot;) Let‚Äôs say that we want to examine the markers for cluster 8 in more detail.\nmarker.set \u0026lt;- markers[[\u0026quot;8\u0026quot;]] as.data.frame(marker.set[1:30,1:3]) ## p.value FDR summary.logFC ## CSTA 7.170624e-222 2.015964e-217 2.4178954 ## MNDA 1.196631e-221 2.015964e-217 2.6614935 ## FCN1 2.375980e-213 2.668543e-209 2.6380934 ## S100A12 4.393470e-212 3.700839e-208 3.0808902 ## VCAN 1.711043e-199 1.153038e-195 2.2603760 ## TYMP 1.173532e-154 6.590164e-151 2.0237930 ## AIF1 3.673649e-149 1.768285e-145 2.4603604 ## LGALS2 4.004740e-137 1.686696e-133 1.8927606 ## MS4A6A 5.639909e-134 2.111457e-130 1.5457061 ## FGL2 2.044513e-124 6.888781e-121 1.3859366 ## RP11-1143G9.4 6.891551e-122 2.110945e-118 2.8042347 ## AP1S2 1.786019e-112 5.014842e-109 1.7703547 ## CD14 1.195352e-110 3.098169e-107 1.4259764 ## CFD 6.870490e-109 1.653531e-105 1.3560255 ## GPX1 9.048825e-107 2.032607e-103 2.4013937 ## TNFSF13B 3.920319e-95 8.255701e-92 1.1151275 ## KLF4 3.309726e-94 6.559876e-91 1.2049050 ## GRN 4.801206e-91 8.987324e-88 1.3814668 ## NAMPT 2.489624e-90 4.415020e-87 1.1438687 ## CLEC7A 7.736088e-88 1.303299e-84 1.0616120 ## S100A8 3.124930e-84 5.013875e-81 4.8051993 ## SERPINA1 1.580359e-82 2.420392e-79 1.3842689 ## CD36 8.018347e-79 1.174653e-75 1.0538169 ## MPEG1 8.481588e-79 1.190744e-75 0.9778095 ## CD68 5.118714e-78 6.898798e-75 0.9481203 ## CYBB 1.200516e-77 1.555776e-74 1.0300245 ## S100A11 1.174556e-72 1.465759e-69 1.8962486 ## RBP7 2.467027e-71 2.968714e-68 0.9666127 ## BLVRB 3.762610e-71 4.371634e-68 0.9701168 ## CD302 9.859086e-71 1.107307e-67 0.8792077 The high expression of MNDA, CD14 and CD68 suggests that cluster 8 contains monocytes.\nmarker.set \u0026lt;- markers[[\u0026quot;15\u0026quot;]] as.data.frame(marker.set[1:30,1:3]) ## p.value FDR summary.logFC ## AIF1 6.912466e-82 2.329086e-77 3.678553 ## FCGR3A 3.606603e-64 6.076045e-60 3.041108 ## SERPINA1 6.149257e-64 6.906436e-60 2.375549 ## FTL 2.857395e-63 2.406927e-59 2.061453 ## LST1 4.161324e-62 2.804233e-58 1.892121 ## COTL1 1.582271e-61 8.885507e-58 1.305713 ## CTSS 3.143428e-55 1.513067e-51 1.880227 ## S100A11 1.719244e-54 7.241024e-51 2.030715 ## TYMP 4.395645e-50 1.645632e-46 2.101758 ## MS4A7 7.348647e-48 2.476053e-44 2.173820 ## NAP1L1 2.806622e-47 8.596939e-44 1.142910 ## FTH1 5.856763e-47 1.644481e-43 1.188781 ## CD68 2.661218e-46 6.897469e-43 2.048530 ## STXBP2 5.783395e-45 1.391898e-41 2.075910 ## SAT1 9.269854e-43 2.082256e-39 1.541614 ## TYROBP 3.459416e-42 7.285097e-39 1.152396 ## PSAP 1.582943e-40 3.137392e-37 1.403571 ## CSTB 3.871723e-40 7.247436e-37 1.129131 ## CFD 4.529809e-39 8.033020e-36 1.999696 ## FKBP1A 5.042155e-39 8.494519e-36 1.301575 ## CEBPB 6.875529e-39 1.103162e-35 1.683662 ## LINC01272 7.357858e-38 1.126889e-34 1.840199 ## FCER1G 2.802003e-37 4.104813e-34 1.301967 ## SPI1 9.236815e-36 1.296772e-32 1.890047 ## S100A4 4.102320e-35 5.528943e-32 4.356299 ## NEAT1 5.928869e-33 7.683358e-30 1.358303 ## IFITM3 6.875461e-33 8.580066e-30 1.871086 ## LYZ 8.944290e-32 1.076317e-28 1.804654 ## LRRC25 1.072482e-31 1.246077e-28 1.242115 ## RNASET2 7.560048e-31 8.490942e-28 1.090212 The high expression of FCGR3A suggests that cluster 15 contains macrophagues.\n# Checking the cluster is what we wanted. marker.set \u0026lt;- markers[[\u0026quot;8\u0026quot;]] topset \u0026lt;- rownames(marker.set)[1:30] stopifnot(all(c(\u0026quot;CD14\u0026quot;, \u0026quot;CD68\u0026quot;, \u0026quot;MNDA\u0026quot;) %in% topset)) plotExpression(sce.pbmc, features=c(\u0026quot;CD14\u0026quot;, \u0026quot;CD68\u0026quot;, \u0026quot;MNDA\u0026quot;, \u0026quot;FCGR3A\u0026quot;), x=\u0026quot;label\u0026quot;, colour_by=\u0026quot;label\u0026quot;)  Figure 6: Distribution of expression values for monocyte and macrophage markers across clusters in the PBMC dataset.   ","date":1613174400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613174400,"objectID":"cfd3544746446c12f7298c1f2515c715","permalink":"/2021-02-13-scrnaseq/","publishdate":"2021-02-13T00:00:00Z","relpermalink":"/2021-02-13-scrnaseq/","section":"post","summary":"Example of scRNAseq workflow.","tags":["r","Bioconductor","scRNAseq"],"title":"Overview of scRNAseq analysis","type":"post"},{"authors":null,"categories":["R"],"content":"   Motivation Standard function to coefficient of variation Modiying the enviroment - no issues Modiying the enviroment - ISSUES! Capturing with enquo Changing name with := Insert a list of expressions into a call Conclusions   Motivation Provide some examples of using Tidy evaluation (also called non-standard evaluation (NSE) or delayed evaluation).\n‚ÑπÔ∏è This post may be useful for your if you have read Chapters 17-20 of Advanced R book and you are looking to find more additional examples of Tidyeval. You may also want to have a look at the great Tidyeval resource put together by Mara.\nLibraries needed for this post\nlibrary(rlang) library(tidyverse) library(testthat)  Standard function to coefficient of variation Let‚Äôs write a function that calculates coefficient of variation:\ncv \u0026lt;- function(var) { sd(var) / mean(var) } We can test that the function is behaving correctly\ntestthat::expect_equal( cv(c(3,3)), 0) testthat::expect_equal( round(cv(c(3,6)),2), 0.47) Great! The function seems to be doing what we want! Capture and uncapture expression We can capture and uncapture expressions with enexpr and bang-bang !!\ncv \u0026lt;- function(var) { var \u0026lt;- enexpr(var) expr(sd(!!var) / mean(!!var)) } There is a lot is going on here:\n enexpr: it captures what the caller supplied to the function and allows delayed evaluation. !!: it unquotes. Sort of like make available what it was captured by enexpr. No evaluation has happened yet. expr: it captures what it was unquoted by !!. No evaluation has happened yet.  Let‚Äôs see what happens:\ntestthat::expect_equal(eval(cv(c(3,3))), 0) testthat::expect_type((cv(c(3,3))), \u0026quot;language\u0026quot;) If we evaluate the function, the cv is equal to 0. Note that we have delayed the evaluation up the point when used eval.\nIf we don‚Äôt evaluate the function it remains as a call object.\n  Modiying the enviroment - no issues Let‚Äôs modify the function‚Äôs environment a little:\ncv \u0026lt;- function(var) { x=6 var \u0026lt;- enexpr(var) expr(sd(!!var) / mean(!!var)) } Let‚Äôs see what happens:\nx=3 testthat::expect_equal( eval(cv(c(3,x))), 0) x=6 testthat::expect_equal(round(eval(cv(c(3,x))),2), 0.47) This works even if we put x in the environment where the function is written because x is not an argument of cv function.\n Modiying the enviroment - ISSUES! adding_cv \u0026lt;- function(df,var) { x=c(3,6) var \u0026lt;- enexpr(var) mutate(df, sd(!!var) / mean(!!var)) } Let‚Äôs see what happens:\ndf\u0026lt;- tibble(n=3) x=c(3,3) adding_cv(df,x) ## # A tibble: 1 x 2 ## n `sd(x)/mean(x)` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 0.471 Wait, what?! The new column contains coefficient of variation. If x=c(3,3) the value in the new column should have been 0. However adding_cv is using x=c(3,6) included in the function environment and not x=c(3,3).\n Capturing with enquo To capture the function and the environment we need enquo\nadding_cv \u0026lt;- function(df,var) { x=c(3,6) var \u0026lt;- enquo(var) mutate(df, sd(!!var) / mean(!!var)) } Let‚Äôs see what happens now:\ndf\u0026lt;- tibble(n=3) x=c(3,3) adding_cv(df,x) ## # A tibble: 1 x 2 ## n `sd(x)/mean(x)` ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 0  Changing name with := We can make the name of the new column prettier with :=\nadding_cv \u0026lt;- function(df,var,nm) { x=c(3,3) nm_name \u0026lt;- quo_name(nm) var \u0026lt;- enquo(var) mutate(df, !!nm_name:= sd(!!var) / mean(!!var)) } Let‚Äôs see if that allows changing the name:\nx=c(3,6) df\u0026lt;- tibble(n=3) adding_cv(df,x,\u0026quot;pretty_name\u0026quot;) ## # A tibble: 1 x 2 ## n pretty_name ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 0.471  Insert a list of expressions into a call What if we want to do delayed filtering? We need to unquote multiple arguments. For this we can use !!!\nadding_cv \u0026lt;- function(df,var,nm, ...) { x=c(3,6) nm_name \u0026lt;- quo_name(nm) var \u0026lt;- enquo(var) filtering \u0026lt;- enquos(...) df %\u0026gt;% filter(!!!filtering) %\u0026gt;% mutate( !!nm_name:= sd(!!var) / mean(!!var))} Let‚Äôs see if we can filter rows 3 and 6 from column n of our df:\ndf\u0026lt;- tibble(n=c(3,6,9),m=c(3,6,9)) x=c(3,3) adding_cv(df,x,\u0026quot;pretty_name\u0026quot;, n %in% c(3,6)) ## # A tibble: 2 x 3 ## n m pretty_name ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 3 3 0 ## 2 6 6 0  Conclusions These were just few examples to illustrate why tidyeval can be useful and when it might be needed. I hope it helped you!\n ","date":1613001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613001600,"objectID":"e23f8b95a2f4b3e49319b60b6221746e","permalink":"/2021-02-11-tidyeval-actions/","publishdate":"2021-02-11T00:00:00Z","relpermalink":"/2021-02-11-tidyeval-actions/","section":"post","summary":"Few examples of how Tidyeval works.","tags":["r","tidyevaluation","metaprogramming","rlang"],"title":"Step-by-step actions in Tidyeval","type":"post"},{"authors":null,"categories":["R"],"content":"   Motivation and data PCA in base R PCA in Tidymodels UMAP in Tidymodels Conclusions   library(tidymodels) library(modeldata) library(ggfortify) library(tidyverse) library(embed) Motivation and data While exploring the modeldata üì¶, I found the dataset oils, which has gas chromatography information used to determine the fatty acid composition of 96 samples corresponding to 7 different vegatable oils of the market. These data is the published work of a chemistry lab. These data is something very close to what we would get in a proteomics lab, and the first thing we tend to do to explore these complex data is to do a PCA to have a simplify idea of its overall distribution in the reduced space.\nEDA data(oils) str(oils) ## tibble [96 √ó 8] (S3: tbl_df/tbl/data.frame) ## $ palmitic : num [1:96] 9.7 11.1 11.5 10 12.2 9.8 10.5 10.5 11.5 10 ... ## $ stearic : num [1:96] 5.2 5 5.2 4.8 5 4.2 5 5 5.2 4.8 ... ## $ oleic : num [1:96] 31 32.9 35 30.4 31.1 43 31.8 31.8 35 30.4 ... ## $ linoleic : num [1:96] 52.7 49.8 47.2 53.5 50.5 39.2 51.3 51.3 47.2 53.5 ... ## $ linolenic : num [1:96] 0.4 0.3 0.2 0.3 0.3 2.4 0.4 0.4 0.2 0.3 ... ## $ eicosanoic: num [1:96] 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 ... ## $ eicosenoic: num [1:96] 0.1 0.1 0.1 0.1 0.1 0.5 0.1 0.1 0.1 0.1 ... ## $ class : Factor w/ 7 levels \u0026quot;corn\u0026quot;,\u0026quot;olive\u0026quot;,..: 4 4 4 4 4 4 4 4 4 4 ... oils %\u0026gt;% count(class) ## # A tibble: 7 x 2 ## class n ## * \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 corn 2 ## 2 olive 7 ## 3 peanut 3 ## 4 pumpkin 37 ## 5 rapeseed 10 ## 6 soybean 11 ## 7 sunflower 26 This looks like fun dataset to project in a reduced dimension space like PCA or UMAP!\n  PCA in base R The steps to generate the components for PCA in base R would be:\npca_res \u0026lt;- oils %\u0026gt;% dplyr::select(where(is.numeric)) %\u0026gt;% # select only the numeric variables tidyr::drop_na() %\u0026gt;% # to drop any NA scale() %\u0026gt;% # to initially normalise the variances prcomp() # to convert numeric data to principal components pca_res ## Standard deviations (1, .., p=7): ## [1] 1.78631393 1.21432295 1.11849881 0.80775705 0.49010697 0.43543634 0.03437479 ## ## Rotation (n x k) = (7 x 7): ## PC1 PC2 PC3 PC4 PC5 ## palmitic -0.1724393 -0.69299469 -0.04593832 0.46972220 -0.19508286 ## stearic -0.4589668 -0.25101419 -0.24289349 0.18544207 0.61204669 ## oleic 0.4578722 -0.39918199 0.14986398 -0.28962122 0.08386290 ## linoleic -0.4590266 0.44858975 -0.11564307 0.05114339 -0.07111895 ## linolenic 0.3446082 0.27607934 0.23426894 0.80580939 -0.02884460 ## eicosanoic 0.1682596 -0.01595516 -0.81991595 0.04591653 -0.46100031 ## eicosenoic 0.4384013 0.14034544 -0.41942317 0.08389933 0.60157904 ## PC6 PC7 ## palmitic -0.4661816 0.10904667 ## stearic 0.5067647 0.03928963 ## oleic 0.2409267 0.67792957 ## linoleic -0.2371904 0.71467174 ## linolenic 0.2916300 0.12220735 ## eicosanoic 0.2889776 0.03216008 ## eicosenoic -0.4929535 0.01587562 We can see that PC componennt for each class of oil were added in a prcomp object.\nAnd we could plot those component with autoplot\nautoplot(pca_res, data = oils, colour = \u0026quot;class\u0026quot;) + labs(color = NULL) +theme_minimal() We can see that this PCA separates olive oil far away from the other 7 types of oils. It also looks like one of the olive oils is closer to peanunt type of oil in the PCA space .\n PCA in Tidymodels Modeling is very much like cooking, and in the Tidymodels universe the language is reflects this very well üë©‚Äçüç≥. There are three things that we will need to do:\n Writing down a recipe üë©‚Äçüç≥ Preparing that recipe üçù Juicing the recipe üçµ  Writing down a recipe We write down the recipe by adding series of steps.\npca_rec \u0026lt;- recipe(~., data = oils) %\u0026gt;% # start writing the recipe with all the data update_role(class, new_role = \u0026quot;id\u0026quot;) %\u0026gt;% # to keep this column around but not include it in the model step_normalize(all_predictors()) %\u0026gt;% # to normalise the data step_pca(all_predictors()) # to convert numeric data to principal components As we see the steps that we need to follow to write the recipe are very similar to the steps followed in base R. However, this is not all. In fact, if we explore how the recipe looks like:\npca_rec ## Data Recipe ## ## Inputs: ## ## role #variables ## id 1 ## predictor 7 ## ## Operations: ## ## Centering and scaling for all_predictors() ## No PCA components were extracted. We can see that the design matrix with id and predictor variables was created. The recipe tells us that the No PCA components were extracted. This is because a recipe specifies what we want to do, but it doesn‚Äôt really do anything to the data yet. We need to extract those components by preparing the recipe.\n Preparing that recipe We can use the function prep for preparing to train this data recipe. Prep returns an updated recipe with the estimates.\npca_prep \u0026lt;- prep(pca_rec) pca_prep ## Data Recipe ## ## Inputs: ## ## role #variables ## id 1 ## predictor 7 ## ## Training data contained 96 data points and no missing data. ## ## Operations: ## ## Centering and scaling for palmitic, stearic, oleic, linoleic, ... [trained] ## PCA extraction with palmitic, stearic, oleic, linoleic, ... [trained] In the operations we see that the data has been [trained].\nGreat! But these are still not the components ü§î. We need to finalise that prepared recipe by juicing it!\n Juicing the recipe We need to apply these operation to the data; juice returns a tibble where all steps have been applied to the data.\npca_juiced \u0026lt;- juice(pca_prep) pca_juiced ## # A tibble: 96 x 6 ## class PC1 PC2 PC3 PC4 PC5 ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 pumpkin -1.22 -0.296 -0.245 -0.158 0.0882 ## 2 pumpkin -1.10 -0.771 -0.198 -0.00964 -0.0901 ## 3 pumpkin -1.08 -1.06 -0.212 0.0154 0.00279 ## 4 pumpkin -1.14 -0.266 -0.192 -0.177 -0.137 ## 5 pumpkin -1.25 -0.995 -0.241 0.226 -0.186 ## 6 pumpkin 0.572 -0.500 -0.0821 0.0652 0.286 ## 7 pumpkin -1.13 -0.530 -0.202 -0.0640 -0.0592 ## 8 pumpkin -1.13 -0.530 -0.202 -0.0640 -0.0592 ## 9 pumpkin -1.08 -1.06 -0.212 0.0154 0.00279 ## 10 pumpkin -1.14 -0.266 -0.192 -0.177 -0.137 ## # ‚Ä¶ with 86 more rows Great! The processed data is ready to ‚Äúconsumed‚Äù by a plot!\npca_juiced %\u0026gt;% ggplot(aes(PC1, PC2, label = class)) + geom_point(aes(color = class), alpha = 0.7, size = 2) + labs(color = NULL) +theme_minimal() The initial PCA and this one generated with Tidymodels look very similar. Note that autoplot adds some information to the plot such as providing PCs percentage. So what‚Äôs the point of using Tidymodels if is a such a long series of steps compared to base R? Well, Tidymodels integrates a lot of modular packages which facilitates creating and evaluating different models.\n  UMAP in Tidymodels In addition to PCA, we could plot a UMAP representation. To do that we would need a new recipe, one that includes a step specify UMAP dimension reduction technique; this step is naturally called step_umap. Once that we have this recipe, the process is the same. Recipe, prep, juice.\numap_rec \u0026lt;- recipe(~., data = oils) %\u0026gt;% update_role(class, new_role = \u0026quot;id\u0026quot;) %\u0026gt;% step_normalize(all_predictors()) %\u0026gt;% step_umap(all_predictors()) # this step makes a different recipe  umap_prep \u0026lt;- prep(umap_rec) umap_juiced \u0026lt;- juice(umap_prep) umap_juiced %\u0026gt;% ggplot(aes(umap_1, umap_2, label = class)) + geom_point(aes(color = class), alpha = 0.7, size = 2) + labs(color = NULL) This model separates the data in the space somewhat differently to PCA. PCA and UMAP are fundamentally different in that PCA is a linear dimensionality reduction algorithm whereas UMAP is non-linear. Moreover, there are few important parameters that can impact how the UMAP representation looks like. This is nicely explained in the README of umapr package from the ropenscilabs. You can see additional arguments offered by step_umap with ?step_umap. Also note that we have trained our models with a tiny set of data (we have not done resampling) and we have not evaluated their performance.\n Conclusions The data processing for doing unsupervised machine learning with Tidymodels are very similar to base R. Linear and non-linear dimensionality reduction algorithms separate the data in the reduced space differently.\n ","date":1612396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612569600,"objectID":"a3862cc77779342ec65a5fb89707dd2f","permalink":"/2021-02-04-unsupervised-machine-learning-with-tidymodels/","publishdate":"2021-02-04T00:00:00Z","relpermalink":"/2021-02-04-unsupervised-machine-learning-with-tidymodels/","section":"post","summary":"Create a recipe, prep it and juice it to do PCA and UMAP with Tidymodels.","tags":["r","modeldata","PCA","UMAP","tidymodels"],"title":"PCA and UMAP classification of vegetable oils with tidymodels \u0026 base R","type":"post"},{"authors":null,"categories":["R"],"content":"   Motivation Statistics of Bioconductor downloads Full details of Bioconductor packages Word network of Bioconductor packages Conclusions   library(BiocPkgTools) library(tidyverse) library(tidytext) library(widyr) library(igraph) library(ggraph) library(lubridate) library(emo) Motivation Bioconductor has a total of 5796 at the present day 2021-01-31. Therefore, navigating across Bioconductor packages can be a daunting experience. Luckily, BiocPkgTools offers a rich ecosystem of metadata around Bioconductor packages üìú.\n Statistics of Bioconductor downloads We can get a tidy data.frame with download stats for all packages using the function biocDownloadStats.\n# Getting a tidy tibble summarizing monthly download statistics bio_download_stats \u0026lt;- biocDownloadStats() bio_download_stats %\u0026gt;% head(13) ## # A tibble: 13 x 7 ## Package Year Month Nb_of_distinct_IPs Nb_of_downloads repo Date ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 ABarray 2021 Jan 54 114 Software 2021-01-01 ## 2 ABarray 2021 Feb 0 0 Software 2021-02-01 ## 3 ABarray 2021 Mar 0 0 Software 2021-03-01 ## 4 ABarray 2021 Apr 0 0 Software 2021-04-01 ## 5 ABarray 2021 May 0 0 Software 2021-05-01 ## 6 ABarray 2021 Jun 0 0 Software 2021-06-01 ## 7 ABarray 2021 Jul 0 0 Software 2021-07-01 ## 8 ABarray 2021 Aug 0 0 Software 2021-08-01 ## 9 ABarray 2021 Sep 0 0 Software 2021-09-01 ## 10 ABarray 2021 Oct 0 0 Software 2021-10-01 ## 11 ABarray 2021 Nov 0 0 Software 2021-11-01 ## 12 ABarray 2021 Dec 0 0 Software 2021-12-01 ## 13 ABarray 2021 all 54 114 Software NA As we see observations for all the months of the year are generated once that the year starts (download values for events in the future are filled up with 0). Also note that there is a summary statistic for month called all embedded inside the tibble, and the Date value for that observation is NA (this would makes group by date very convenient).\nThis tibble contains information about packages that expands from 2009 to 2021. There are 3 categories of packages, with the total number of package per category as follows:\nbio_download_stats %\u0026gt;% distinct(Package, repo) %\u0026gt;% count(repo) %\u0026gt;% knitr::kable()   repo n    AnnotationData 2659  ExperimentData 821  Software 2316     Full details of Bioconductor packages The complete information for the packages as described in the DESCRIPTION file can be obtained with biocPkgList.\nbpi = biocPkgList() colnames(bpi) ## [1] \u0026quot;Package\u0026quot; \u0026quot;Version\u0026quot; \u0026quot;Depends\u0026quot; ## [4] \u0026quot;Suggests\u0026quot; \u0026quot;License\u0026quot; \u0026quot;MD5sum\u0026quot; ## [7] \u0026quot;NeedsCompilation\u0026quot; \u0026quot;Title\u0026quot; \u0026quot;Description\u0026quot; ## [10] \u0026quot;biocViews\u0026quot; \u0026quot;Author\u0026quot; \u0026quot;Maintainer\u0026quot; ## [13] \u0026quot;git_url\u0026quot; \u0026quot;git_branch\u0026quot; \u0026quot;git_last_commit\u0026quot; ## [16] \u0026quot;git_last_commit_date\u0026quot; \u0026quot;Date/Publication\u0026quot; \u0026quot;source.ver\u0026quot; ## [19] \u0026quot;win.binary.ver\u0026quot; \u0026quot;mac.binary.ver\u0026quot; \u0026quot;vignettes\u0026quot; ## [22] \u0026quot;vignetteTitles\u0026quot; \u0026quot;hasREADME\u0026quot; \u0026quot;hasNEWS\u0026quot; ## [25] \u0026quot;hasINSTALL\u0026quot; \u0026quot;hasLICENSE\u0026quot; \u0026quot;Rfiles\u0026quot; ## [28] \u0026quot;dependencyCount\u0026quot; \u0026quot;Imports\u0026quot; \u0026quot;Enhances\u0026quot; ## [31] \u0026quot;dependsOnMe\u0026quot; \u0026quot;VignetteBuilder\u0026quot; \u0026quot;suggestsMe\u0026quot; ## [34] \u0026quot;LinkingTo\u0026quot; \u0026quot;Archs\u0026quot; \u0026quot;URL\u0026quot; ## [37] \u0026quot;SystemRequirements\u0026quot; \u0026quot;BugReports\u0026quot; \u0026quot;importsMe\u0026quot; ## [40] \u0026quot;Video\u0026quot; \u0026quot;linksToMe\u0026quot; \u0026quot;OS_type\u0026quot; ## [43] \u0026quot;PackageStatus\u0026quot; \u0026quot;License_restricts_use\u0026quot; \u0026quot;License_is_FOSS\u0026quot; ## [46] \u0026quot;organism\u0026quot; There is lots of information in here. We could use this metadata information to understand the connections between packages.\n Word network of Bioconductor packages The most informative variables about the packages are Title and Description so we can explore the connections between packages doing some text mining using a Tidytext approach.\nTo prepare our dataset we need to initially tokenize the text. The Wikipedia definition for tokenization on lexical analysis is as follows:\n Tokenization is the process of demarcating and possibly classifying sections of a string of input characters\n The sections can be words, sentence, ngram or chapter (for example if analysis a book). In this case we are gonna break down package Titles or Description into words using the function unnest_tokens. In addition, we can remove stop words (included in the Tidytext dataset).\nbpi_title \u0026lt;- bpi %\u0026gt;% dplyr::select(Package, Title) %\u0026gt;% unnest_tokens(word, Title) %\u0026gt;% anti_join(stop_words) bpi_description \u0026lt;- bpi %\u0026gt;% dplyr::select(Package, Description) %\u0026gt;% unnest_tokens(word, Description) %\u0026gt;% anti_join(stop_words) Note that the number of words from Title is 11932 and the number of words from Description is 59370, so package Descriptions contain on average 5 times the words of package Titles.\nWe can have a look at how the tokenised titles for each package look like:\nbpi_title ## # A tibble: 11,932 x 2 ## Package word ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 a4 automated ## 2 a4 affymetrix ## 3 a4 array ## 4 a4 analysis ## 5 a4 umbrella ## 6 a4 package ## 7 a4Base automated ## 8 a4Base affymetrix ## 9 a4Base array ## 10 a4Base analysis ## # ‚Ä¶ with 11,922 more rows Them, we can use pairwise_count from the widyr package to count how many times each pair of words occurs together in the package Title. This function works as a mutate in that it takes the variables to compare and returns a tibble with the pairwise columns and an extra column called n containing the number of words co-occurrences. I think this function is very sweet üçØ!\nbpi_titlepairs \u0026lt;- bpi_title %\u0026gt;% pairwise_count(Package, word, sort = TRUE, upper = FALSE) bpi_desciptionpairs \u0026lt;- bpi_description %\u0026gt;% pairwise_count(Package, word, sort = TRUE, upper = FALSE) This data is ready for visualization of network of co-occurring words in package Titles. We can use the ggraph package for visualizing this network. We are going to represent just the top co-occurring words, or otherwise we get a very populated network which is impossible to read.\nset.seed(1234) bpi_titlepairs %\u0026gt;% filter(n \u0026gt;= 6) %\u0026gt;% graph_from_data_frame() %\u0026gt;% ggraph(layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \u0026quot;purple\u0026quot;) + geom_node_point(size = 5) + geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \u0026quot;lines\u0026quot;)) + theme_void()+ theme(legend.position=\u0026quot;none\u0026quot;)+ labs(title = \u0026quot; Number of word co-ocurrences in packages titles\u0026quot;)  Figure 1: Word network in Bioconductor packages Titles  We see some clear and logical clustering of packages in this network.For example, DESEq and DESeq2 packages cluster together, as one would expect since they DESeq2 is the successor of DESeq. There are other obvious connections such as MSstatsTMTPTM and MSstatsTMTP since the former has added functionality to analyse PTMs on TMT shotgun mass spectrometry-based proteomic experiments. There is a big cluster on the bottom left corner with packages to analyse RNASeq and single cell RNASeq.\nWhat about the network build from words of the Description?\nset.seed(1234) bpi_desciptionpairs %\u0026gt;% filter(n \u0026gt;= 15) %\u0026gt;% graph_from_data_frame() %\u0026gt;% ggraph(layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \u0026quot;orange\u0026quot;) + geom_node_point(size = 5) + geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \u0026quot;lines\u0026quot;)) + theme_void()+ theme(legend.position=\u0026quot;none\u0026quot;)+ labs(title = \u0026quot;Number of word co-ocurrences in packages Description\u0026quot;)  Figure 2: Word network in Bioconductor packages Description  We see more connections here, and some of the relationships are still obvious (e.g HiCcompare and multiHiCcompare, anota and anota2seq, AnnotationHub and ExperimentHub). This network is richer, and one would have to dive a bit deeper to get a better sense of this network.\n Conclusions Text mining of Bioconductor packages metadata is a straight forward visual way to understand the relationships between packages. One could go beyond this and for example finding words that are especially important across package Descriptions by calculating tf-idf statistic. One could also set up a GitHub Action executed as a CRON job to get updates periodically. This could turn into a challenge for BiocChallenges. This post was inspired by Chapter 8 of the Tidytext book and BiocRoulette.\n ","date":1611964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611964800,"objectID":"de5a8f5f1137357769655b78fa06b145","permalink":"/2021-01-30-network-visualization-of-bioconductor-packages/","publishdate":"2021-01-30T00:00:00Z","relpermalink":"/2021-01-30-network-visualization-of-bioconductor-packages/","section":"post","summary":"Understanding how Bioconductor packages are connected between each other using metadata.","tags":["r","BiocPkgTools","Bioconductor","Tidytext","widyr","ggraph"],"title":"Word network of Bioconductor packages","type":"post"},{"authors":null,"categories":["R"],"content":"   Fundations of my motivation Why I decided to become an RStudio Tidyverse During the training Between training and exam During the teaching exam During the Tidyverse exam Voil√†!   Fundations of my motivation Early 2020, in a pre-pandemic world, I attended my first Rladies meetup in London by Mine Cetinkaya-Rundel. Part of the reasons I decided to attended was because my husband was going to be away for the foreseeable months, so I decided to give myself the chance to connect with other Rladies. It was only few months before that meetup that I started using the Tidyverse, although I used R for many years in the past. It was a fantastic talk where I met some very cool Rladies and I got super impressed by Mine‚Äôs teaching abilities.\nDuring the past year, and because of continous lockdowns and WFH, I really dived into the Tidyverse: I joint LatinR talks, participated in Rladies Barcelona Florence Nightingale content, I watched Tidytuesday screencasts, opened a blog with Hugo, Rblogdown and Netlify (thanks to the amazing resources shared by Alison Hill). I could easily say that the Tidyverse mission üõ∞ keep my mental health afloat during such difficult times.\nMoreover, during the summer of 2020 I got some formal teaching training from university of London where I learn concepts like Blooms taxonomy and I started to give by-weekly workshops for biology researcher co-workers of mine where I introduce them to ggplot2 and statistical concepts such as correlation.\nAll these things had a direct impact on my research career; for instance I published my first R package and gave my first EuroBioconductor short talk.\n Why I decided to become an RStudio Tidyverse I decided to get certificated for several reasons:\nGetting additional formal training on how to teach. I felt I and my audience could benefit more the better trained for training I would be. Pushing myself to really know inside out the Tidyverse. Getting the training forced me to read the R4DS book and doing the exercises thoroughly. I used additional fantastic resources such as https://datasciencebox.org/. Later, I learn that there are book clubs for some of the most popular R books, something that I somewhat missed, but I am very excited to join in 2021 the tidymodels book club organised by Jon Harmon.  The RStudio Instructor training journey has properly equipped me to do and teach data science! This process pushed me out of my comfort zone for which I will be forever grateful.\n During the training  Instructor training covering modern teaching methods. Materials for this training are available here  This was a two half days course where Greg Wilson lead such inspiring modules. What I like the most was feeling part of the learning community, the interactivity of the modules and genuinely learning about evidence-based effective teaching methods.\n Be nice, the rest is on the details ‚ÄìGreg\n  Between training and exam Materials Books. Reading the book Teaching Tech Together written by Greg. The book is currently undergoing translation into Spanish by a team of volunteers and I am honored to be among them thanks to Yanina Bellini Saibene.\n Guides. Going through R4DS instructor‚Äôs guide which has learning objectives and key points for each chapter of R4DS book.\n Concept maps. Building concept maps to design lessons taking into account cognitive load is a big part of the training. When preparing for the two exams I found this resource extremely useful to check I was building an accurate mental model of the topics covered in R4DS. I built myself quite a lot of mental models that you can find here\n  Greg also asked us to send an email with what we remember and so I did this concept map (there is a lot of information buried beneath my bad hand writing)\nWatching and reading other people‚Äôs experiences.   Process to become a certified Rstudio instructor a video where some Mi-useRs (minority R users) talk about their experiences and motivations to become a certified Rstudio instructor.\n Obtaining RStudio certification. A shared path a post by Yanina on her journey to obtain RStudio certification.\n Reflections by Brendan Cullen‚Äôs on RStudio Instructor Training .\n Overview by Silvia Canelon of the RStudio Instructor certification process. Check her post since she has an amazing collection of resources to support anyone on their certification journey.\n  Active learning .   With some of the materials I use during my learning, I created some exercises using the learnr package, which is such a powerful and versatile way to create interactive exercises. This is my humble contribution to fading exercises with Penguins and I hope to extend these materials in user!2021 later this year\n I also gave a mocked presentation to Riva Quiroga and Yanina Bellini over Zoom and received extremely useful feedback. Gracias chicas!!\n    During the teaching exam A 90-minute teaching exam based on content from the instructor training. This includes a 15-minute demonstration lesson and a written component. Mine was on RStudio projects and my learner persona was Lovorka. You can see my contribution here.   During the Tidyverse exam A 90-minute Tidyverse exam to test your knowledge of the subject matter in R for Data Science (make sure to check the program FAQs for information about the Shiny exam). I hope to share my exercise soon.   Voil√†! Joining this community is a dream‚Ä¶ that may become true for you too? üçé\n ","date":1611100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611100800,"objectID":"2755b1ed5ecb6f2c75135d08b5f69ca0","permalink":"/2021-01-20-rstudio-instructor-certification-tidyverse/","publishdate":"2021-01-20T00:00:00Z","relpermalink":"/2021-01-20-rstudio-instructor-certification-tidyverse/","section":"post","summary":"Sharing my experience on the process to obtain [RStudio Instructor certification](https://education.rstudio.com/trainers/#info).","tags":["r","tidyverse"],"title":"Joining the RStudio Tidyverse Instructor community","type":"post"},{"authors":null,"categories":["R"],"content":"   Introduction Getting the data Quetting auxiliary annotation (Biomart Query) Nearest neighbour transfer learning Conclusions   # loading libraries # clearing environment bc https://support.bioconductor.org/p/p132709/ rm(list = ls()) library(pRoloc) library(pRolocdata) library(BiocStyle) Introduction Within the cell, the localization of a given protein is determined by its biological function. Subcellular proteomics is the method to study protein sub-cellular localization in a systematic manner. There are two complementary ways to analysis localized proteins:\n On one hand biochemical sub-cellular fractionation experiments allow empirical quantification of protein across sub-cellular and sub-organellar compartments. Proteins are allocated to a given subcellular niche if the detected intensity is higher than a threshold. We can say that this type of data has high signal-to-noise ratio, but is available in limited amounts (primary data).\n On the other hand, databases such as GO contain large amount of information about sub-cellular proteins localisation, but this is information is blended for a many biological systems. We can say that this type of data has high low signal-to-noise, but is available in large amounts (auxiliary data).\n  So we want to know how to optimally combine primary and auxiliary data.üíπ\nTo do so, we need to weight both types of data. If we imagine a multivariate distribution (like a Dirichlet distribution) were all the components take values between (0,1) and all values sum up to one, we can imagine that a weight of 1 indicates that the final annotation rely exclusively on the experimental data and ignore completely the auxiliary data and a weight of 0 represents the opposite situation, where the primary data is ignored and only the auxiliary data is considered.\nWe could use a transfer learning algorithm to efficiently complement the primary data with auxiliary data without compromising the integrity of the former. This is implemented in the pRoloc package and it was published by Breckels et al and expanded by Crook et al using a Bayesian approach. In this post I will step-by-step walk through KNN transfer learning.\n Getting the data We start with HEK293T2011 proteomics data available in the pRolocdata package.\ndata(HEK293T2011) The class of (HEK293T2011) isMSnSet instance, an efficient and established to store and handle MS data and metainformation efficiently. I am not going to discuss much about this class of objects since the field is moving towards other types of data storage such as SummarizedExperiment objects\nWe can also get an overview experimental data and query how many proteins across how many conditions were quantified.\nhead(exprs(HEK293T2011),2) ## X113 X114 X115 X116 X117 X118 X119 ## O00767 0.1360547 0.1495961 0.10623931 0.1465050 0.2773137 0.14294025 0.03796970 ## P51648 0.1914456 0.2052463 0.05661169 0.1651138 0.2366302 0.09964387 0.01803788 ## X121 ## O00767 0.003381233 ## P51648 0.027270640 dim(exprs(HEK293T2011)) ## [1] 1371 8 What is important to know is that 1371 proteins were quantified across eight iTRAQ 8-plex labelled fractions ( one could know a bit more about the experiment with ?HEK293T2011)\nNext thing we can do is see how well these organelles have been resolved in the experiment using a PCA plot\nplot2D(HEK293T2011) addLegend(HEK293T2011, where = \u0026quot;topright\u0026quot;, cex = 0.6)  Figure 1: PCA plot of HEK293T2011 subcellular proteomics dataset  We see that some organelles such as cytosol and cytosol/nucleus are well resolved - and so they will get a high weigh- while others such as the Golgi or the ER are less so - so they will get a low weight. There are some proteins that do not get annotation because the resolution of the experiment did not allow so.\n Quetting auxiliary annotation (Biomart Query) Next thing we can do is get auxiliary data. We can do so by querying biomaRt and storing the annotation as an AnnotationParams object. Again, this is part of the pRoloc package, and it has been created for efficient data handling.\nap \u0026lt;- setAnnotationParams(inputs = c(\u0026quot;Human genes\u0026quot;, \u0026quot;UniProtKB/Swiss-Prot ID\u0026quot;)) ## Connecting to Biomart... We can access this instance with\ngetAnnotationParams() ## Object of class \u0026quot;AnnotationParams\u0026quot; ## Using the \u0026#39;ENSEMBL_MART_ENSEMBL\u0026#39; BioMart database ## Using the \u0026#39;hsapiens_gene_ensembl\u0026#39; dataset ## Using \u0026#39;uniprotswissprot\u0026#39; as filter ## Created on Tue Jan 26 15:47:22 2021 We can annotate our innitial HEK293T2011 data by creating a new MSnSet instance populated with a GO term as a binary matrix (so the auxiliary data with information about 889 cellular compartment GO terms has been added).\nHEK293T2011goset \u0026lt;- makeGoSet(HEK293T2011)  Nearest neighbour transfer learning Deciding the weight We could define more or less weight values between 0 and 1 depending on how granular we want to be with our search (more weight will give finer-grained integration).For example for 3 classes, 3 weights will generate:\ngtools::permutations(length(seq(0, 1, 0.5)), 3, seq(0, 1, 0.5), repeats.allowed = TRUE)  ## [,1] [,2] [,3] ## [1,] 0.0 0.0 0.0 ## [2,] 0.0 0.0 0.5 ## [3,] 0.0 0.0 1.0 ## [4,] 0.0 0.5 0.0 ## [5,] 0.0 0.5 0.5 ## [6,] 0.0 0.5 1.0 ## [7,] 0.0 1.0 0.0 ## [8,] 0.0 1.0 0.5 ## [9,] 0.0 1.0 1.0 ## [10,] 0.5 0.0 0.0 ## [11,] 0.5 0.0 0.5 ## [12,] 0.5 0.0 1.0 ## [13,] 0.5 0.5 0.0 ## [14,] 0.5 0.5 0.5 ## [15,] 0.5 0.5 1.0 ## [16,] 0.5 1.0 0.0 ## [17,] 0.5 1.0 0.5 ## [18,] 0.5 1.0 1.0 ## [19,] 1.0 0.0 0.0 ## [20,] 1.0 0.0 0.5 ## [21,] 1.0 0.0 1.0 ## [22,] 1.0 0.5 0.0 ## [23,] 1.0 0.5 0.5 ## [24,] 1.0 0.5 1.0 ## [25,] 1.0 1.0 0.0 ## [26,] 1.0 1.0 0.5 ## [27,] 1.0 1.0 1.0 As we sayed before, HEK293T2011goset experiment has 10 subcellular compartments, and so the total combinations for 10 classes, 4 weights will be:\nth \u0026lt;- gtools::permutations(length(seq(0, 1, length.out = 4)), 10, seq(0, 1, length.out = 4), repeats.allowed = TRUE) Total combination of weights for HEK293T2011goset experiment will be 1048576.\npRoloc package comes with a convenient function thetas to produce such a weight matrix (because we need a theta for each of the training feature).\n## marker classes for HEK293T2011 m \u0026lt;- unique(fData(HEK293T2011)$markers.tl) m \u0026lt;- m[m != \u0026quot;unknown\u0026quot;] th \u0026lt;- thetas(length(m), length.out=4)  Optimizing weigth We can do a grid search to determine which is the best th, with the knntlOptimisation function of the pRoloc package.\ntopt \u0026lt;- knntlOptimisation(HEK293T2011, HEK293T2011goset, th = th, k = c(3, 3), fcol = \u0026quot;markers.tl\u0026quot;, times = 50, method = \u0026quot;Breckels\u0026quot; ) For the sake of time, we can reduce our initial data, as it will take a long time to do this grid search (even if knntlOptimisation uses parallelisation by default).\nset.seed(2021) i \u0026lt;- sample(nrow(th), 50) topt \u0026lt;- knntlOptimisation(HEK293T2011, HEK293T2011goset, th = th[i, ], k = c(3, 3), fcol = \u0026quot;markers.tl\u0026quot;, times = 5) The optimisation is performed on the labelled marker examples only. topt result stores all the result from the optimisation step, and in particular the observed theta weights, which can be directly plotted as shown on the bubble plot.\nplot(topt) Result stocores for the optimisation step. Note that this figure is the result using extensive optimisation on the whole HEK293T2011 dataset and auxiliary HEK293T2011goset dataset, not only with only a random subset of 50 candidate weights.\n We see that the cytosol and cytosol/nucleus and ER are predominantly scored with high heights, consistent with high reliability of the primary data. Golgi, PM and the 40S ribosomal clusters are scored with smaller scores, indicating a substantial benefit of the auxiliary data.\nThe best grid search parameters can be accessed with:\ngetParams(topt) Note that the data HEK293T2011 gets annotated with the best parameters at the knntlOptimisation step. We can get the best weights with:\nbw \u0026lt;- experimentData(HEK293T2011)@other$knntl$thetas  Performing transfer learning To apply the best weights and learn from the auxiliary data to classify the unlabelled proteins into sub-cellular niches (present in markers.tl column), we can pass the primary and auxiliary data sets, best weights, best k‚Äôs and the metadata feature data taht contains the markers definitions to the knntlClassification function.\nHEK293T2011 \u0026lt;- knntlClassification(HEK293T2011, HEK293T2011goset, bestTheta = bw, k = c(3, 3), fcol = \u0026quot;markers.tl\u0026quot;) In this step, annotation predictors scores and parameters get added into the MSnSet data. We can access the predicted localization conveniently using the getPredictions assessor.\nHEK293T2011 \u0026lt;- getPredictions(HEK293T2011, fcol = \u0026quot;knntl\u0026quot;)  Plotting the results # These functions allow to get/set the colours/points to plot organelle features setStockcol(paste0(getStockcol(), \u0026quot;80\u0026quot;)) #this defines the point size ptsze \u0026lt;- exp(fData(HEK293T2011)$knntl.scores) - 1 plot2D(HEK293T2011, fcol = \u0026quot;knntl\u0026quot;, cex = ptsze) setStockcol(NULL) addLegend(HEK293T2011, where = \u0026quot;topright\u0026quot;, fcol = \u0026quot;markers.tl\u0026quot;, bty = \u0026quot;n\u0026quot;, cex = .7) PCA plot of HEK293T2011 after transfer learning classification. The size of the points is proportional to the classification scores.\n   Conclusions A weighted k-nearest neighbour transfer learning algorithm can be very useful to predict of protein sub-cellular localisation using quantitative proteomics data as primary data source and Gene Ontology-derived binary data as auxiliary data source.\n ","date":1611100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611446400,"objectID":"e4ed91b7673eaa0fbd23e17d7a6ff9e1","permalink":"/2021-01-20-transfer-learning-for-spatial-proteomics/","publishdate":"2021-01-20T00:00:00Z","relpermalink":"/2021-01-20-transfer-learning-for-spatial-proteomics/","section":"post","summary":"Exploration of how a transfer learning algorithm can predict proteins sub-cellular localisation.","tags":["r","computational proteomics","bioconductor"],"title":"Transfer learning for spatial proteomics","type":"post"},{"authors":null,"categories":["R"],"content":"  ","date":1607904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607904000,"objectID":"4743bd42544eeef683b4372790118dd3","permalink":"/talk/2020-11-26-eurobioc/","publishdate":"2020-11-26T00:00:00Z","relpermalink":"/talk/2020-11-26-eurobioc/","section":"talk","summary":"Talk on [PeCorA](https://github.com/jessegmeyerlab/PeCorA) given at [European Bioconductor Meeting 2020](https://eurobioc2020.bioconductor.org/).","tags":["R","proteomics"],"title":"EuroBioc","type":"talk"},{"authors":null,"categories":["R"],"content":"  Presentation slides  Selected talk  Talk slides: Peptide Correlation Analysis (PeCorA) Reveals Differential Proteoform Regulation    ","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607040000,"objectID":"cb9303e25f4aad736ebf1d8ac9f8e8e8","permalink":"/talk/2020-11-21-iscb/","publishdate":"2020-11-21T00:00:00Z","relpermalink":"/talk/2020-11-21-iscb/","section":"talk","summary":"Talk on [PeCorA](https://github.com/jessegmeyerlab/PeCorA) given at [Virtual RSG Belgium Student Symposium](http://www.rsg-belgium.iscbsc.org/event/student-symposium/student-symposium-04-12-2020/).","tags":["R","proteomics"],"title":"ISCB","type":"talk"},{"authors":["Maria Dermit"],"categories":[],"content":"  Abstract Translation of ribosomal protein-coding mRNAs (RP-mRNAs) constitutes a key step in ribosome biogenesis, but the mechanisms that modulate RP-mRNA translation in coordination with other cellular processes are poorly defined. Here, we show that subcellular localization of RP-mRNAs acts as a key regulator of their translation during cell migration. As cells migrate into their surroundings, RP-mRNAs localize to the actin-rich cell protrusions. This localization is mediated by La-related protein 6 (LARP6), an RNA-binding protein that is enriched in protrusions. Protrusions act as hotspots of translation for RP-mRNAs, enhancing RP synthesis, ribosome biogenesis, and the overall protein synthesis in migratory cells. In human breast carcinomas, epithelial-to-mesenchymal transition (EMT) upregulates LARP6 expression to enhance protein synthesis and support invasive growth. Our findings reveal LARP6-mediated mRNA localization as a key regulator of ribosome biogenesis during cell migration and demonstrate a role for this process in cancer progression downstream of EMT.\n2020 RPmRNA Poster (letter)\n ","date":1604880000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604880000,"objectID":"c125874717c915e5c2d44880efdc18f2","permalink":"/publication/maddie-ehr-delivery-episode-algorithm/","publishdate":"2020-11-09T00:00:00Z","relpermalink":"/publication/maddie-ehr-delivery-episode-algorithm/","section":"publication","summary":"**Abstract** Translation of ribosomal protein-coding mRNAs (RP-mRNAs) constitutes a key step in ribosome biogenesis, but the mechanisms that modulate RP-mRNA translation in coordination with other cellular processes are poorly defined. Here, we show that subcellular localization of RP-mRNAs acts as a key regulator of their translation during cell migration. As cells migrate into their surroundings, RP-mRNAs localize to the actin-rich cell protrusions. This localization is mediated by La-related protein 6 (LARP6), an RNA-binding protein that is enriched in protrusions. Protrusions act as hotspots of translation for RP-mRNAs, enhancing RP synthesis, ribosome biogenesis, and the overall protein synthesis in migratory cells. In human breast carcinomas, epithelial-to-mesenchymal transition (EMT) upregulates LARP6 expression to enhance protein synthesis and support invasive growth. Our findings reveal LARP6-mediated mRNA localization as a key regulator of ribosome biogenesis during cell migration and demonstrate a role for this process in cancer progression downstream of EMT.\n","tags":["Ribosome","RPmRNA","Proteomics","Cancer"],"title":"Subcellular mRNA Localization Regulates Ribosome Biogenesis in Migrating Cells","type":"publication"},{"authors":[],"categories":[],"content":"Welcome to Slides  Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"}]