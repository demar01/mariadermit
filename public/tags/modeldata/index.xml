<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>modeldata | Maria Dermit</title>
    <link>/tags/modeldata/</link>
      <atom:link href="/tags/modeldata/index.xml" rel="self" type="application/rss+xml" />
    <description>modeldata</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>¬© Maria Dermit, {2020}</copyright><lastBuildDate>Thu, 04 Feb 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu89765a528fbe3e89fbea23b995128687_292554_512x512_fill_lanczos_center_2.png</url>
      <title>modeldata</title>
      <link>/tags/modeldata/</link>
    </image>
    
    <item>
      <title>PCA and UMAP classification of vegetable oils with tidymodels</title>
      <link>/2021-02-04-unsupervised-machine-learning/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/2021-02-04-unsupervised-machine-learning/</guid>
      <description>
&lt;script src=&#34;../../2021-02-04-unsupervised-machine-learning/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation-and-data&#34;&gt;Motivation and data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pca-in-base-r&#34;&gt;PCA in base R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pca-in-tidymodels&#34;&gt;PCA in Tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#umap-in-tidymodels&#34;&gt;UMAP in Tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
library(modeldata)
library(ggfortify)
library(tidyverse)
library(embed)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;motivation-and-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation and data&lt;/h1&gt;
&lt;p&gt;Exploring the modeldata üì¶, I found this dataset that has GC ‚Ä¶ This is something very similar to what I would get in the lab and the first thing to do when getting this kind of data is to do a PCA of it to get an idea‚Ä¶&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(oils)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(oils)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [96 √ó 8] (S3: tbl_df/tbl/data.frame)
##  $ palmitic  : num [1:96] 9.7 11.1 11.5 10 12.2 9.8 10.5 10.5 11.5 10 ...
##  $ stearic   : num [1:96] 5.2 5 5.2 4.8 5 4.2 5 5 5.2 4.8 ...
##  $ oleic     : num [1:96] 31 32.9 35 30.4 31.1 43 31.8 31.8 35 30.4 ...
##  $ linoleic  : num [1:96] 52.7 49.8 47.2 53.5 50.5 39.2 51.3 51.3 47.2 53.5 ...
##  $ linolenic : num [1:96] 0.4 0.3 0.2 0.3 0.3 2.4 0.4 0.4 0.2 0.3 ...
##  $ eicosanoic: num [1:96] 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 ...
##  $ eicosenoic: num [1:96] 0.1 0.1 0.1 0.1 0.1 0.5 0.1 0.1 0.1 0.1 ...
##  $ class     : Factor w/ 7 levels &amp;quot;corn&amp;quot;,&amp;quot;olive&amp;quot;,..: 4 4 4 4 4 4 4 4 4 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oils %&amp;gt;% 
  count(class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   class         n
## * &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;
## 1 corn          2
## 2 olive         7
## 3 peanut        3
## 4 pumpkin      37
## 5 rapeseed     10
## 6 soybean      11
## 7 sunflower    26&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-in-base-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PCA in base R&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_res&amp;lt;-oils %&amp;gt;% 
     dplyr::select(where(is.numeric)) %&amp;gt;% #select only the numeric variables
     tidyr::drop_na() %&amp;gt;% #to drop any NA 
     scale() %&amp;gt;% #to initially normalise the variances
     prcomp() #to convert numeric data to principal components&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(pca_res, data = oils, colour = &amp;#39;class&amp;#39;) +
    labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../2021-02-04-unsupervised-machine-learning/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
We can see that this PCA separates olive oil far away from the other 7 types of oils. It also looks like one of the olive oils seem to be closer to peanunt type of oil.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-in-tidymodels&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PCA in Tidymodels&lt;/h1&gt;
&lt;p&gt;Modeling is very much like cooking, and in the Tidymodels universe the language is reflects this very well.There are three things that we will need to do:
- Writing down a recipe
- Preparing that recipe
- Juicing the recipe&lt;/p&gt;
&lt;div id=&#34;writing-down-a-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing down a recipe&lt;/h2&gt;
&lt;p&gt;We write down the recipe by adding series of steps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_rec &amp;lt;- recipe(~., data = oils) %&amp;gt;% #start writing the recipe with all the data
  update_role(class, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% #to keep this column around but not include it in the model
  step_normalize(all_predictors()) %&amp;gt;% #to normalise the data
  step_pca(all_predictors()) #to convert numeric data to principal components&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we see the steps that we need to follow to write the recipe are very similar to the steps followed in base R.
However, this is not all. In fact, if we explore the recipe&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_rec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##         id          1
##  predictor          7
## 
## Operations:
## 
## Centering and scaling for all_predictors()
## No PCA components were extracted.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that &lt;code&gt;No PCA components were extracted.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We need to extract those components by preparing the recipe&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-that-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparing that recipe&lt;/h2&gt;
&lt;p&gt;We can use the function &lt;code&gt;prep&lt;/code&gt; for preparing to train this data recipe. Prep returns an updated recipe with the estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_prep &amp;lt;- prep(pca_rec)
pca_prep&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##         id          1
##  predictor          7
## 
## Training data contained 96 data points and no missing data.
## 
## Operations:
## 
## Centering and scaling for palmitic, stearic, oleic, linoleic, ... [trained]
## PCA extraction with palmitic, stearic, oleic, linoleic, ... [trained]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the operations we see that the data has been [trained].&lt;/p&gt;
&lt;p&gt;Great! But these are still not the components ü§î. We need to finalise that prepared recipe by juicing it!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;juicing-the-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Juicing the recipe&lt;/h2&gt;
&lt;p&gt;We need to apply these operation to the data; &lt;code&gt;juice&lt;/code&gt;returns a tibble where all steps have been applied to the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_juiced &amp;lt;-juice(pca_prep)
pca_juiced&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 96 x 6
##    class      PC1    PC2     PC3      PC4      PC5
##    &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 pumpkin -1.22  -0.296 -0.245  -0.158    0.0882 
##  2 pumpkin -1.10  -0.771 -0.198  -0.00964 -0.0901 
##  3 pumpkin -1.08  -1.06  -0.212   0.0154   0.00279
##  4 pumpkin -1.14  -0.266 -0.192  -0.177   -0.137  
##  5 pumpkin -1.25  -0.995 -0.241   0.226   -0.186  
##  6 pumpkin  0.572 -0.500 -0.0821  0.0652   0.286  
##  7 pumpkin -1.13  -0.530 -0.202  -0.0640  -0.0592 
##  8 pumpkin -1.13  -0.530 -0.202  -0.0640  -0.0592 
##  9 pumpkin -1.08  -1.06  -0.212   0.0154   0.00279
## 10 pumpkin -1.14  -0.266 -0.192  -0.177   -0.137  
## # ‚Ä¶ with 86 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! The processed data is ready to ‚Äúconsumed‚Äù by a plot!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_juiced %&amp;gt;%
    ggplot(aes(PC1, PC2, label = class)) +
    geom_point(aes(color = class), alpha = 0.7, size = 2) +
    labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../2021-02-04-unsupervised-machine-learning/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The initial PCA and this one generated with Tidymodels look very similar. Note that autoplot takes care of several data transformation such as providing the percentage‚Ä¶ (maybe plot pca_res with ggplot and not autoplot?)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;umap-in-tidymodels&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;UMAP in Tidymodels&lt;/h1&gt;
&lt;p&gt;To plot a UMAP representation we need a new recipe, one that does has a step to include UMAP. This step is naturally called &lt;code&gt;step_umap&lt;/code&gt;, and creates a specification of a recipe step that will project a set of features into a smaller space. Once that we have this recipe, the process is the same. Recipe, prep, juice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_rec &amp;lt;- recipe(~., data = oils) %&amp;gt;%
  update_role(class, new_role = &amp;quot;id&amp;quot;) %&amp;gt;%
  step_normalize(all_predictors()) %&amp;gt;%
   step_umap(all_predictors())  # this step makes a different recipe &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_prep &amp;lt;- prep(umap_rec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_juiced &amp;lt;-juice(umap_prep)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_juiced %&amp;gt;%
  ggplot(aes(umap_1, umap_2, label = class)) +
  geom_point(aes(color = class), alpha = 0.7, size = 2) + 
  labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This model separates the data in the space somewhat differently. PCA and UMAP are fundamentally different in that
PCA is a linear dimensionality reduction algorithm (and tend to perform faster), whereas
UMAP is non-linear. Moreover, there are few important parameters that can impact how the UMAP representation looks like as nicely shown in the umapr package README from the &lt;a href=&#34;https://github.com/ropenscilabs/umapr&#34;&gt;ropenscilabs&lt;/a&gt;. You can see additional arguments offered by &lt;code&gt;step_umap&lt;/code&gt; with &lt;code&gt;?step_umap&lt;/code&gt;. Also note that we have trained our models with a tiny set of data (we have not done resampling) and we have neither measured the performance of the models nor compared them side by side in a stack of models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;The steps for doing unsupervised machine learning with Tidymodels are very similar to base R. Linear and non-linear dimensionality reduction algorithms separate the data in the reduced space differently. Resampling and model comparison could help to formally determine which model performs better.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
