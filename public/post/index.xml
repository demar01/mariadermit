<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | English site</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Maria Dermit, {2020}</copyright><lastBuildDate>Sat, 13 Feb 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu89765a528fbe3e89fbea23b995128687_292554_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Overview of scRNAseq analysis</title>
      <link>/2021-02-13-scrnaseq/</link>
      <pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/2021-02-13-scrnaseq/</guid>
      <description>
&lt;script src=&#34;../2021-02-13-scrnaseq/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-loading&#34;&gt;Data loading&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#qc&#34;&gt;QC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#normalization&#34;&gt;Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#variance-modellingfeature-selection&#34;&gt;Variance modelling/feature selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dimensionality-reduction&#34;&gt;Dimensionality reduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#clustering&#34;&gt;Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation&#34;&gt;Interpretation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;The aim of this post is to show an overview of the common framework for analyzing Single-Cell RNA-seq (scRNAseq) data. Before diving in the data analysis, let’s look at the library prep on a 10X Genomics machine for scRNAseq:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://medicine.uiowa.edu/humangenetics/genomics-sequencing-division/genome-sequencing/single-cell-expression-analysis-scrna-seq&#34;&gt;&lt;img src=&#34;10X.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Dr. Eric Chow gives a fantastic overview of how cells and barcodes are sorted with the Droseq method (&lt;a href=&#34;https://www.youtube.com/watch?v=k9VFNLLQP8c&amp;amp;t=1083s&#34;&gt;min 7:08&lt;/a&gt;).&lt;br /&gt;
&lt;br /&gt;
Once the data is generated the sequential steps to handle scRNAseq are summarized in this carton from Bioconductor site:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;scrnaseqframework.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The central object of the pipeline is &lt;code&gt;SingleCellExperiment&lt;/code&gt;, which holds all the raw data and experiment information, and it looks like this:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/OSCABioc2019__OSCABioc2019/&#34;&gt;&lt;img src=&#34;SingleCellExperiment.png&#34; alt=&#34;SingleCellExperiment object&#34; /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://bioconductor.org/books/release/OSCA/&#34;&gt;OSCA book&lt;/a&gt; is an amazing resource to fully understand the math behind scRNAseq analysis workflow. It has 18 cases studies of scRNAseq.&lt;/p&gt;
&lt;p&gt;The main steps followed in scRNAseq analysis are the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Data Loading&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;QC&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Normalization&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Variance modeling&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Dimensionality reduction&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Clustering&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Interpretation&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s dive in each step to understand what happens at each stage.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-loading&#34; class=&#34;section level1&#34; style=&#34;color: black&#34;&gt;
&lt;h1&gt;Data loading&lt;/h1&gt;
&lt;p&gt;Here, I am going to use an example of 10X Genomics data the counts matrix and associated metadata (cell barcodes, data path, etc.). This is based on &lt;a href=&#34;http://bioconductor.org/books/release/OSCA/unfiltered-human-pbmcs-10x-genomics.html&#34;&gt;Chapter 27 in OSCA book&lt;/a&gt;. The &lt;a href=&#34;https://bioconductor.org/packages/3.12/DropletTestFiles&#34;&gt;&lt;em&gt;DropletTestFiles&lt;/em&gt;&lt;/a&gt; package contains files that are the raw output of pipelines like 10X Genomics’ CellRanger software suite, that can be then imported via the &lt;a href=&#34;https://bioconductor.org/packages/3.12/DropletUtils&#34;&gt;&lt;em&gt;DropletUtils&lt;/em&gt;&lt;/a&gt; package’s &lt;code&gt;read10xCounts()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DropletTestFiles)
raw.path &amp;lt;- getTestFile(&amp;quot;tenx-2.1.0-pbmc4k/1.0.0/raw.tar.gz&amp;quot;)
out.path &amp;lt;- file.path(tempdir(), &amp;quot;pbmc4k&amp;quot;)
untar(raw.path, exdir=out.path)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(DropletUtils)
fname &amp;lt;- file.path(out.path, &amp;quot;raw_gene_bc_matrices/GRCh38&amp;quot;)
sce.pbmc &amp;lt;- read10xCounts(fname, col.names=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need to make sure that we convert the easy interpretable gene symbol to a standard identifier that is guaranteed to be unique and valid (e.g., Ensembl).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(scater)
rownames(sce.pbmc) &amp;lt;- uniquifyFeatureNames(
    rowData(sce.pbmc)$ID, rowData(sce.pbmc)$Symbol)
library(EnsDb.Hsapiens.v86)
location &amp;lt;- mapIds(EnsDb.Hsapiens.v86, keys=rowData(sce.pbmc)$ID, 
    column=&amp;quot;SEQNAME&amp;quot;, keytype=&amp;quot;GENEID&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In addition to this, the &lt;a href=&#34;https://bioconductor.org/packages/release/data/experiment/vignettes/scRNAseq/inst/doc/scRNAseq.html&#34;&gt;scRNAseq package&lt;/a&gt; provides convenient access to several publicly available data sets in the form of &lt;code&gt;SingleCellExperiment&lt;/code&gt; object.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(scRNAseq)
out &amp;lt;- listDatasets()
dim(out)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 46  5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(out)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## DataFrame with 6 rows and 5 columns
##                Reference  Taxonomy                 Part    Number
##              &amp;lt;character&amp;gt; &amp;lt;integer&amp;gt;          &amp;lt;character&amp;gt; &amp;lt;integer&amp;gt;
## 1 @aztekin2019identifi..      8355                 tail     13199
## 2 @bach2017differentia..     10090        mammary gland     25806
## 3   @baron2016singlecell      9606             pancreas      8569
## 4   @baron2016singlecell     10090             pancreas      1886
## 5 @buettner2015computa..     10090 embryonic stem cells       288
## 6 @campbell2017molecular     10090                brain     21086
##                     Call
##              &amp;lt;character&amp;gt;
## 1      AztekinTailData()
## 2      BachMammaryData()
## 3 BaronPancreasData(&amp;#39;h..
## 4 BaronPancreasData(&amp;#39;m..
## 5      BuettnerESCData()
## 6    CampbellBrainData()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;qc&#34; class=&#34;section level1&#34; style=&#34;color:black&#34;&gt;
&lt;h1&gt;QC&lt;/h1&gt;
&lt;p&gt;An unique aspect of droplet-based data is that we have no prior knowledge about whether a particular library (i.e., cell barcode) corresponds to cell-containing or empty droplets. We use the &lt;code&gt;emptyDrops()&lt;/code&gt; function to test whether the expression profile for each cell barcode is significantly different from the ambient RNA pool. &lt;code&gt;emptyDrops()&lt;/code&gt; assumes that barcodes with low total UMI counts are empty droplets. &lt;code&gt;emptyDrops()&lt;/code&gt; uses Monte Carlo simulations to compute p-values for the multinomial sampling transcripts from the ambient pool.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100) #seed forreproducible results, emptyDrops performas a simulation.
e.out &amp;lt;- emptyDrops(counts(sce.pbmc))
sce.pbmc &amp;lt;- sce.pbmc[,which(e.out$FDR &amp;lt;= 0.001)] #we exclude dropplets that have low UMI counts &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are the unfiltered, non-empty drops&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;unfiltered &amp;lt;- sce.pbmc&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each cell, we calculate these QC metrics using the &lt;code&gt;perCellQCMetrics()&lt;/code&gt; function from the &lt;a href=&#34;https://bioconductor.org/packages/3.12/scater&#34;&gt;&lt;em&gt;scater&lt;/em&gt;&lt;/a&gt; package. The &lt;code&gt;sum&lt;/code&gt; column contains the total count for each cell and the &lt;code&gt;detected&lt;/code&gt; column contains the number of detected genes. Mitochondrial counts are informative, because small mitochondrial percentages, large spike-in percentages and small library sizes are likely to be stripped nuclei, i.e., they have been so extensively damaged that they have lost all cytoplasmic content, therefore are low quality cells. Here we use a relaxed QC strategy and only remove cells with large mitochondrial proportions, using it as a proxy for cell damage. This reduces the risk of removing cell types with low RNA content, especially in a heterogeneous PBMC population with many different cell types.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stats &amp;lt;- perCellQCMetrics(sce.pbmc, subsets=list(Mito=which(location==&amp;quot;MT&amp;quot;)))
high.mito &amp;lt;- isOutlier(stats$subsets_Mito_percent, type=&amp;quot;higher&amp;quot;)
sce.pbmc &amp;lt;- sce.pbmc[,!high.mito]&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(high.mito)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Mode   FALSE    TRUE 
## logical    3985     315&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colData(unfiltered) &amp;lt;- cbind(colData(unfiltered), stats)
unfiltered$discard &amp;lt;- high.mito

gridExtra::grid.arrange(
    plotColData(unfiltered, y=&amp;quot;sum&amp;quot;, colour_by=&amp;quot;discard&amp;quot;) +
        scale_y_log10() + ggtitle(&amp;quot;Total count&amp;quot;),
    plotColData(unfiltered, y=&amp;quot;detected&amp;quot;, colour_by=&amp;quot;discard&amp;quot;) +
        scale_y_log10() + ggtitle(&amp;quot;Detected features&amp;quot;),
    plotColData(unfiltered, y=&amp;quot;subsets_Mito_percent&amp;quot;,
        colour_by=&amp;quot;discard&amp;quot;) + ggtitle(&amp;quot;Mito percent&amp;quot;),
    ncol=2
)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unref-unfiltered-pbmc-qc&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-02-13-scrnaseq/index_files/figure-html/unref-unfiltered-pbmc-qc-1.png&#34; alt=&#34;Distribution of various QC metrics in the PBMC dataset after cell calling. Each point is a cell and is colored according to whether it was discarded by the mitochondrial filter.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Distribution of various QC metrics in the PBMC dataset after cell calling. Each point is a cell and is colored according to whether it was discarded by the mitochondrial filter.
&lt;/p&gt;
&lt;/div&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotColData(unfiltered, x=&amp;quot;sum&amp;quot;, y=&amp;quot;subsets_Mito_percent&amp;quot;,
    colour_by=&amp;quot;discard&amp;quot;) + scale_x_log10()&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unref-unfiltered-pbmc-mito&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-02-13-scrnaseq/index_files/figure-html/unref-unfiltered-pbmc-mito-1.png&#34; alt=&#34;Proportion of mitochondrial reads in each cell of the PBMC dataset compared to its total count.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Proportion of mitochondrial reads in each cell of the PBMC dataset compared to its total count.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;normalization&#34; class=&#34;section level1&#34; style=&#34;color:black&#34;&gt;
&lt;h1&gt;Normalization&lt;/h1&gt;
&lt;p&gt;There are a number of &lt;a href=&#34;http://bioconductor.org/books/release/OSCA/normalization.html&#34;&gt;normalization methods&lt;/a&gt;. We use a pre-clustering step with &lt;code&gt;quickCluster()&lt;/code&gt; where cells in each cluster are normalized separately and the size factors are rescaled to be comparable across clusters. This avoids the assumption that most genes are non-DE across the entire population - only a non-DE majority is required between pairs of clusters, which is a weaker assumption for highly heterogeneous populations. By default, &lt;code&gt;quickCluster()&lt;/code&gt; will use an approximate algorithm for PCA based on methods from the &lt;a href=&#34;https://cran.r-project.org/package=irlba&#34;&gt;&lt;em&gt;irlba&lt;/em&gt;&lt;/a&gt; package. The approximation relies on stochastic initialization so we need to set the random seed (via &lt;code&gt;set.seed()&lt;/code&gt;) for reproducibility. &lt;code&gt;computeSumFactors&lt;/code&gt; performs a scaling normalization of single-cell RNA-seq data by deconvolving size factors from cell pools. Once we have computed the size factors, we use the &lt;code&gt;logNormCounts()&lt;/code&gt; function from &lt;a href=&#34;https://bioconductor.org/packages/3.12/scater&#34;&gt;&lt;em&gt;scater&lt;/em&gt;&lt;/a&gt; to compute normalized expression values for each cell. This is done by dividing the count for each gene/spike-in transcript with the appropriate size factor for that cell. The function also log-transforms the normalized values, creating a new assay called &lt;code&gt;&#34;logcounts&#34;&lt;/code&gt;. (Technically, these are “log-transformed normalized expression values”, but that’s too much of a mouthful to fit into the assay name.) These log-values will be the basis of our downstream analyses.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(scran)
set.seed(1000)
clusters &amp;lt;- quickCluster(sce.pbmc)
sce.pbmc &amp;lt;- computeSumFactors(sce.pbmc, cluster=clusters)
sce.pbmc &amp;lt;- logNormCounts(sce.pbmc)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sizeFactors(sce.pbmc))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##  0.00749  0.71207  0.87490  1.00000  1.09900 12.25412&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can plot The “library size factor” for each cell. As for bulk RNAseq, library
size normalization is the simplest strategy for performing scaling normalization.
However, library size normalization is usually sufficient in many applications where the aim is to identify clusters and the top markers that define each cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(librarySizeFactors(sce.pbmc), sizeFactors(sce.pbmc), pch=16,
    xlab=&amp;quot;Library size factors&amp;quot;, ylab=&amp;quot;Deconvolution factors&amp;quot;, log=&amp;quot;xy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unref-unfiltered-pbmc-norm&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-02-13-scrnaseq/index_files/figure-html/unref-unfiltered-pbmc-norm-1.png&#34; alt=&#34;Relationship between the library size factors and the deconvolution size factors in the PBMC dataset.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Relationship between the library size factors and the deconvolution size factors in the PBMC dataset.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;variance-modellingfeature-selection&#34; class=&#34;section level1&#34; style=&#34;color:black&#34;&gt;
&lt;h1&gt;Variance modelling/feature selection&lt;/h1&gt;
&lt;p&gt;Several methods are available to quantify the variation per gene and to select an appropriate set of &lt;strong&gt;highly variable genes (HVGs)&lt;/strong&gt;. UMI counts typically exhibit near-Poisson variation if we only consider technical noise from library preparation and sequencing. This can be used to construct a mean-variance trend in the log-counts with the &lt;code&gt;modelGeneVarByPoisson&lt;/code&gt; function. We can then select the top 10% of genes with the highest biological components.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1001)
dec.pbmc &amp;lt;- modelGeneVarByPoisson(sce.pbmc)
top.pbmc &amp;lt;- getTopHVGs(dec.pbmc, prop=0.1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(dec.pbmc$mean, dec.pbmc$total, pch=16, cex=0.5,
    xlab=&amp;quot;Mean of log-expression&amp;quot;, ylab=&amp;quot;Variance of log-expression&amp;quot;)
curfit &amp;lt;- metadata(dec.pbmc)
curve(curfit$trend(x), col=&amp;#39;dodgerblue&amp;#39;, add=TRUE, lwd=2)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unref-unfiltered-pbmc-var&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-02-13-scrnaseq/index_files/figure-html/unref-unfiltered-pbmc-var-1.png&#34; alt=&#34;Per-gene variance as a function of the mean for the log-expression values in the PBMC dataset. Each point represents a gene (black) with the mean-variance trend (blue) fitted to simulated Poisson counts.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Per-gene variance as a function of the mean for the log-expression values in the PBMC dataset. Each point represents a gene (black) with the mean-variance trend (blue) fitted to simulated Poisson counts.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;dimensionality-reduction&#34; class=&#34;section level1&#34; style=&#34;color:black&#34;&gt;
&lt;h1&gt;Dimensionality reduction&lt;/h1&gt;
&lt;p&gt;Now we want to compare cells based on the values of gene expression. We can use
&lt;code&gt;denoisePCA&lt;/code&gt; function, that “de-noises” log-expression data by removing principal components corresponding to technical noise.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(10000)
sce.pbmc &amp;lt;- denoisePCA(sce.pbmc, subset.row=top.pbmc, technical=dec.pbmc)
set.seed(100000)
sce.pbmc &amp;lt;- runTSNE(sce.pbmc, dimred=&amp;quot;PCA&amp;quot;) 
set.seed(1000000)
sce.pbmc &amp;lt;- runUMAP(sce.pbmc, dimred=&amp;quot;PCA&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We verify how many PCs were retained.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ncol(reducedDim(sce.pbmc, &amp;quot;PCA&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;clustering&#34; class=&#34;section level1&#34; style=&#34;color:black&#34;&gt;
&lt;h1&gt;Clustering&lt;/h1&gt;
&lt;p&gt;We can now create nearest-neighbor graphs with the &lt;code&gt;buildSNNGraph&lt;/code&gt; function and
&lt;code&gt;cluster_walktrap&lt;/code&gt; function. &lt;code&gt;cluster_walktrap&lt;/code&gt; will assign membership of densely connected subgraphs, also called “communities”.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- buildSNNGraph(sce.pbmc, k=10, use.dimred = &amp;#39;PCA&amp;#39;)
clust &amp;lt;- igraph::cluster_walktrap(g)$membership
colLabels(sce.pbmc) &amp;lt;- factor(clust)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(colLabels(sce.pbmc))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
## 205 508 541  56 374 125  46 432 302 867  47 155 166  61  84  16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotTSNE(sce.pbmc, colour_by=&amp;quot;label&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unref-unfiltered-pbmc-tsne&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-02-13-scrnaseq/index_files/figure-html/unref-unfiltered-pbmc-tsne-1.png&#34; alt=&#34;Obligatory $t$-SNE plot of the PBMC dataset, where each point represents a cell and is colored according to the assigned cluster.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Obligatory &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-SNE plot of the PBMC dataset, where each point represents a cell and is colored according to the assigned cluster.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interpretation&lt;/h1&gt;
&lt;p&gt;To interpret the clustering results, we need to identify the genes that drive separation between clusters. In the most obvious case, the marker genes for each cluster are a priori associated with particular cell types, allowing us to treat the clustering as a proxy for cell type identity. The same principle can be applied to discover more subtle differences between clusters (e.g., changes in activation or differentiation state) based on the behavior of genes in the affected pathways.
We perform pairwise t-tests between clusters for each gene using the &lt;code&gt;findMarkers&lt;/code&gt; function, which uses a Welch t-test to perform DE testing between clusters and returns a list of DataFrames containing ranked candidate markers for each cluster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;markers &amp;lt;- findMarkers(sce.pbmc, pval.type=&amp;quot;some&amp;quot;, direction=&amp;quot;up&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s say that we want to examine the markers for cluster 8 in more detail.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marker.set &amp;lt;- markers[[&amp;quot;8&amp;quot;]]
as.data.frame(marker.set[1:30,1:3])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     p.value           FDR summary.logFC
## CSTA          7.170624e-222 2.015964e-217     2.4178954
## MNDA          1.196631e-221 2.015964e-217     2.6614935
## FCN1          2.375980e-213 2.668543e-209     2.6380934
## S100A12       4.393470e-212 3.700839e-208     3.0808902
## VCAN          1.711043e-199 1.153038e-195     2.2603760
## TYMP          1.173532e-154 6.590164e-151     2.0237930
## AIF1          3.673649e-149 1.768285e-145     2.4603604
## LGALS2        4.004740e-137 1.686696e-133     1.8927606
## MS4A6A        5.639909e-134 2.111457e-130     1.5457061
## FGL2          2.044513e-124 6.888781e-121     1.3859366
## RP11-1143G9.4 6.891551e-122 2.110945e-118     2.8042347
## AP1S2         1.786019e-112 5.014842e-109     1.7703547
## CD14          1.195352e-110 3.098169e-107     1.4259764
## CFD           6.870490e-109 1.653531e-105     1.3560255
## GPX1          9.048825e-107 2.032607e-103     2.4013937
## TNFSF13B       3.920319e-95  8.255701e-92     1.1151275
## KLF4           3.309726e-94  6.559876e-91     1.2049050
## GRN            4.801206e-91  8.987324e-88     1.3814668
## NAMPT          2.489624e-90  4.415020e-87     1.1438687
## CLEC7A         7.736088e-88  1.303299e-84     1.0616120
## S100A8         3.124930e-84  5.013875e-81     4.8051993
## SERPINA1       1.580359e-82  2.420392e-79     1.3842689
## CD36           8.018347e-79  1.174653e-75     1.0538169
## MPEG1          8.481588e-79  1.190744e-75     0.9778095
## CD68           5.118714e-78  6.898798e-75     0.9481203
## CYBB           1.200516e-77  1.555776e-74     1.0300245
## S100A11        1.174556e-72  1.465759e-69     1.8962486
## RBP7           2.467027e-71  2.968714e-68     0.9666127
## BLVRB          3.762610e-71  4.371634e-68     0.9701168
## CD302          9.859086e-71  1.107307e-67     0.8792077&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The high expression of &lt;em&gt;MNDA&lt;/em&gt;, &lt;em&gt;CD14&lt;/em&gt; and &lt;em&gt;CD68&lt;/em&gt; suggests that cluster 8 contains monocytes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;marker.set &amp;lt;- markers[[&amp;quot;15&amp;quot;]]
as.data.frame(marker.set[1:30,1:3])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                p.value          FDR summary.logFC
## AIF1      6.912466e-82 2.329086e-77      3.678553
## FCGR3A    3.606603e-64 6.076045e-60      3.041108
## SERPINA1  6.149257e-64 6.906436e-60      2.375549
## FTL       2.857395e-63 2.406927e-59      2.061453
## LST1      4.161324e-62 2.804233e-58      1.892121
## COTL1     1.582271e-61 8.885507e-58      1.305713
## CTSS      3.143428e-55 1.513067e-51      1.880227
## S100A11   1.719244e-54 7.241024e-51      2.030715
## TYMP      4.395645e-50 1.645632e-46      2.101758
## MS4A7     7.348647e-48 2.476053e-44      2.173820
## NAP1L1    2.806622e-47 8.596939e-44      1.142910
## FTH1      5.856763e-47 1.644481e-43      1.188781
## CD68      2.661218e-46 6.897469e-43      2.048530
## STXBP2    5.783395e-45 1.391898e-41      2.075910
## SAT1      9.269854e-43 2.082256e-39      1.541614
## TYROBP    3.459416e-42 7.285097e-39      1.152396
## PSAP      1.582943e-40 3.137392e-37      1.403571
## CSTB      3.871723e-40 7.247436e-37      1.129131
## CFD       4.529809e-39 8.033020e-36      1.999696
## FKBP1A    5.042155e-39 8.494519e-36      1.301575
## CEBPB     6.875529e-39 1.103162e-35      1.683662
## LINC01272 7.357858e-38 1.126889e-34      1.840199
## FCER1G    2.802003e-37 4.104813e-34      1.301967
## SPI1      9.236815e-36 1.296772e-32      1.890047
## S100A4    4.102320e-35 5.528943e-32      4.356299
## NEAT1     5.928869e-33 7.683358e-30      1.358303
## IFITM3    6.875461e-33 8.580066e-30      1.871086
## LYZ       8.944290e-32 1.076317e-28      1.804654
## LRRC25    1.072482e-31 1.246077e-28      1.242115
## RNASET2   7.560048e-31 8.490942e-28      1.090212&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The high expression of &lt;em&gt;FCGR3A&lt;/em&gt; suggests that cluster 15 contains macrophagues.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Checking the cluster is what we wanted.
marker.set &amp;lt;- markers[[&amp;quot;8&amp;quot;]]
topset &amp;lt;- rownames(marker.set)[1:30]
stopifnot(all(c(&amp;quot;CD14&amp;quot;, &amp;quot;CD68&amp;quot;, &amp;quot;MNDA&amp;quot;) %in% topset))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plotExpression(sce.pbmc, features=c(&amp;quot;CD14&amp;quot;, &amp;quot;CD68&amp;quot;,
    &amp;quot;MNDA&amp;quot;, &amp;quot;FCGR3A&amp;quot;), x=&amp;quot;label&amp;quot;, colour_by=&amp;quot;label&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unref-mono-pbmc-markers&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-02-13-scrnaseq/index_files/figure-html/unref-mono-pbmc-markers-1.png&#34; alt=&#34;Distribution of expression values for monocyte and macrophage markers across clusters in the PBMC dataset.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Distribution of expression values for monocyte and macrophage markers across clusters in the PBMC dataset.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Step-by-step actions in Tidyeval</title>
      <link>/2021-02-11-tidyeval-actions/</link>
      <pubDate>Thu, 11 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/2021-02-11-tidyeval-actions/</guid>
      <description>
&lt;script src=&#34;../2021-02-11-tidyeval-actions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#standard-function-to-coefficient-of-variation&#34;&gt;Standard function to coefficient of variation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modiying-the-enviroment---no-issues&#34;&gt;Modiying the enviroment - no issues&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modiying-the-enviroment---issues&#34;&gt;Modiying the enviroment - ISSUES!&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#capturing-with-enquo&#34;&gt;Capturing with enquo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#changing-name-with&#34;&gt;Changing name with :=&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#insert-a-list-of-expressions-into-a-call&#34;&gt;Insert a list of expressions into a call&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Provide some examples of using &lt;a href=&#34;https://tidyeval.tidyverse.org/&#34;&gt;Tidy evaluation&lt;/a&gt; (also called non-standard evaluation (NSE) or &lt;strong&gt;delayed evaluation&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;ℹ️ This post may be useful for your if you have read &lt;a href=&#34;https://adv-r.hadley.nz/metaprogramming.html&#34;&gt;Chapters 17-20 of Advanced R book&lt;/a&gt; and you are looking to find more additional examples of Tidyeval. You may also want to have a look at the great Tidyeval resource put together by &lt;a href=&#34;https://maraaverick.rbind.io/2017/08/tidyeval-resource-roundup/&#34;&gt;Mara&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Libraries needed for this post&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rlang)
library(tidyverse)
library(testthat)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-function-to-coefficient-of-variation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Standard function to coefficient of variation&lt;/h1&gt;
&lt;p&gt;Let’s write a function that calculates coefficient of variation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv &amp;lt;- function(var) {
    sd(var) / mean(var)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can test that the function is behaving correctly&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testthat::expect_equal( cv(c(3,3)), 0)
testthat::expect_equal( round(cv(c(3,6)),2), 0.47)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! The function seems to be doing what we want!
&lt;img src=&#34;https://media.giphy.com/media/12FwhN6Qh3cfxm/giphy.gif&#34; alt=&#34;Baby steps&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;capture-and-uncapture-expression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Capture and uncapture expression&lt;/h2&gt;
&lt;p&gt;We can capture and uncapture expressions with &lt;a href=&#34;https://adv-r.hadley.nz/quasiquotation.html&#34;&gt;&lt;code&gt;enexpr&lt;/code&gt; and bang-bang !!&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv &amp;lt;- function(var) {
    var &amp;lt;- enexpr(var) 
    expr(sd(!!var) / mean(!!var)) 
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is a lot is going on here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;enexpr&lt;/strong&gt;: it captures what the caller supplied to the function and allows delayed evaluation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;!!&lt;/strong&gt;: it unquotes. Sort of like make available what it was captured by enexpr. No evaluation has happened yet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;expr&lt;/strong&gt;: it captures what it was unquoted by !!. No evaluation has happened yet.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let’s see what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;testthat::expect_equal(eval(cv(c(3,3))), 0) 
testthat::expect_type((cv(c(3,3))), &amp;quot;language&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we evaluate the function, the cv is equal to 0. Note that we &lt;strong&gt;have delayed the evaluation&lt;/strong&gt; up the point when used eval.&lt;/p&gt;
&lt;p&gt;If we don’t evaluate the function it remains as a &lt;a href=&#34;https://adv-r.hadley.nz/expressions.html#calls&#34;&gt;call object&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/ChzfTLSi47FYc/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;modiying-the-enviroment---no-issues&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modiying the enviroment - no issues&lt;/h1&gt;
&lt;p&gt;Let’s modify the function’s environment a little:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cv &amp;lt;- function(var) {
    x=6
    var &amp;lt;- enexpr(var)
    expr(sd(!!var) / mean(!!var))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x=3
testthat::expect_equal( eval(cv(c(3,x))), 0)
x=6
testthat::expect_equal(round(eval(cv(c(3,x))),2), 0.47)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This works even if we put x in the environment where the function is written because x is not an argument of cv function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modiying-the-enviroment---issues&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modiying the enviroment - ISSUES!&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adding_cv &amp;lt;- function(df,var) {
    x=c(3,6)
    var &amp;lt;- enexpr(var)
    mutate(df, sd(!!var) / mean(!!var))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what happens:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df&amp;lt;- tibble(n=3)
x=c(3,3)
adding_cv(df,x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##       n `sd(x)/mean(x)`
##   &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;
## 1     3           0.471&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wait, what?! The new column contains coefficient of variation. If x=c(3,3) the value in the new column should have been 0. However adding_cv is using x=c(3,6) included in the function environment and not x=c(3,3).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/d9rGLfxrh1Rfy/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;capturing-with-enquo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Capturing with enquo&lt;/h1&gt;
&lt;p&gt;To capture the function and the environment we need &lt;a href=&#34;https://adv-r.hadley.nz/evaluation.html?q=enquo#creating&#34;&gt;enquo&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adding_cv &amp;lt;- function(df,var) {
    x=c(3,6)
    var &amp;lt;- enquo(var)
    mutate(df, sd(!!var) / mean(!!var))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see what happens now:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df&amp;lt;- tibble(n=3)
x=c(3,3)
adding_cv(df,x)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##       n `sd(x)/mean(x)`
##   &amp;lt;dbl&amp;gt;           &amp;lt;dbl&amp;gt;
## 1     3               0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/jsm9GubElH2X04lRNn/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;changing-name-with&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Changing name with :=&lt;/h1&gt;
&lt;p&gt;We can make the name of the new column prettier with &lt;a href=&#34;https://adv-r.hadley.nz/quasiquotation.html#tidy-dots&#34;&gt;&lt;strong&gt;:=&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adding_cv &amp;lt;- function(df,var,nm) {
    x=c(3,3)
    nm_name &amp;lt;- quo_name(nm)
    var &amp;lt;- enquo(var)
    mutate(df, !!nm_name:= sd(!!var) / mean(!!var))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see if that allows changing the name:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x=c(3,6)
df&amp;lt;- tibble(n=3)
adding_cv(df,x,&amp;quot;pretty_name&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##       n pretty_name
##   &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1     3       0.471&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/Zb63DjukhzkT1d0VoZ/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;insert-a-list-of-expressions-into-a-call&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Insert a list of expressions into a call&lt;/h1&gt;
&lt;p&gt;What if we want to do &lt;em&gt;delayed&lt;/em&gt; filtering? We need to unquote multiple arguments. For this we can use &lt;a href=&#34;https://adv-r.hadley.nz/quasiquotation.html#unquoting-many-arguments&#34;&gt;!!!&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adding_cv &amp;lt;- function(df,var,nm, ...) {
    x=c(3,6)
    nm_name &amp;lt;- quo_name(nm)
    var &amp;lt;- enquo(var)
    filtering &amp;lt;- enquos(...)
    df %&amp;gt;%  filter(!!!filtering) %&amp;gt;% 
    mutate( !!nm_name:= sd(!!var) / mean(!!var))}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s see if we can filter rows 3 and 6 from column n of our df:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df&amp;lt;- tibble(n=c(3,6,9),m=c(3,6,9))
x=c(3,3)
adding_cv(df,x,&amp;quot;pretty_name&amp;quot;, n %in% c(3,6))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##       n     m pretty_name
##   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1     3     3           0
## 2     6     6           0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/xTiTnCf8IJvjldRaxi/giphy.gif&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;These were just few examples to illustrate why &lt;a href=&#34;%5Bhttps://tidyeval.tidyverse.org/&#34;&gt;tidyeval&lt;/a&gt; can be useful and when it might be needed. I hope it helped you!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PCA and UMAP classification of vegetable oils with tidymodels &amp; base R</title>
      <link>/2021-02-04-unsupervised-machine-learning-with-tidymodels/</link>
      <pubDate>Thu, 04 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/2021-02-04-unsupervised-machine-learning-with-tidymodels/</guid>
      <description>
&lt;script src=&#34;../2021-02-04-unsupervised-machine-learning-with-tidymodels/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation-and-data&#34;&gt;Motivation and data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pca-in-base-r&#34;&gt;PCA in base R&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pca-in-tidymodels&#34;&gt;PCA in Tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#umap-in-tidymodels&#34;&gt;UMAP in Tidymodels&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels)
library(modeldata)
library(ggfortify)
library(tidyverse)
library(embed)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;motivation-and-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation and data&lt;/h1&gt;
&lt;p&gt;While exploring the &lt;a href=&#34;https://github.com/tidymodels/modeldata&#34;&gt;modeldata&lt;/a&gt; 📦, I found the dataset &lt;code&gt;oils&lt;/code&gt;, which has gas chromatography information used to determine the fatty acid composition of 96 samples corresponding to 7 different vegatable oils of the market. These data is the &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0169743904001200&#34;&gt;published work&lt;/a&gt; of a chemistry lab. These data is something very close to what we would get in a proteomics lab, and the first thing we tend to do to explore these complex data is to do a PCA to have a simplify idea of its overall distribution in the reduced space.&lt;/p&gt;
&lt;div id=&#34;eda&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;EDA&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(oils)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(oils)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## tibble [96 × 8] (S3: tbl_df/tbl/data.frame)
##  $ palmitic  : num [1:96] 9.7 11.1 11.5 10 12.2 9.8 10.5 10.5 11.5 10 ...
##  $ stearic   : num [1:96] 5.2 5 5.2 4.8 5 4.2 5 5 5.2 4.8 ...
##  $ oleic     : num [1:96] 31 32.9 35 30.4 31.1 43 31.8 31.8 35 30.4 ...
##  $ linoleic  : num [1:96] 52.7 49.8 47.2 53.5 50.5 39.2 51.3 51.3 47.2 53.5 ...
##  $ linolenic : num [1:96] 0.4 0.3 0.2 0.3 0.3 2.4 0.4 0.4 0.2 0.3 ...
##  $ eicosanoic: num [1:96] 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 ...
##  $ eicosenoic: num [1:96] 0.1 0.1 0.1 0.1 0.1 0.5 0.1 0.1 0.1 0.1 ...
##  $ class     : Factor w/ 7 levels &amp;quot;corn&amp;quot;,&amp;quot;olive&amp;quot;,..: 4 4 4 4 4 4 4 4 4 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;oils %&amp;gt;%
  count(class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 2
##   class         n
## * &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;
## 1 corn          2
## 2 olive         7
## 3 peanut        3
## 4 pumpkin      37
## 5 rapeseed     10
## 6 soybean      11
## 7 sunflower    26&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks like fun dataset to project in a reduced dimension space like PCA or UMAP!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-in-base-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PCA in base R&lt;/h1&gt;
&lt;p&gt;The steps to generate the components for PCA in base R would be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_res &amp;lt;- oils %&amp;gt;%
  dplyr::select(where(is.numeric)) %&amp;gt;% # select only the numeric variables
  tidyr::drop_na() %&amp;gt;% # to drop any NA
  scale() %&amp;gt;% # to initially normalise the variances
  prcomp() # to convert numeric data to principal components&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_res&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Standard deviations (1, .., p=7):
## [1] 1.78631393 1.21432295 1.11849881 0.80775705 0.49010697 0.43543634 0.03437479
## 
## Rotation (n x k) = (7 x 7):
##                   PC1         PC2         PC3         PC4         PC5
## palmitic   -0.1724393 -0.69299469 -0.04593832  0.46972220 -0.19508286
## stearic    -0.4589668 -0.25101419 -0.24289349  0.18544207  0.61204669
## oleic       0.4578722 -0.39918199  0.14986398 -0.28962122  0.08386290
## linoleic   -0.4590266  0.44858975 -0.11564307  0.05114339 -0.07111895
## linolenic   0.3446082  0.27607934  0.23426894  0.80580939 -0.02884460
## eicosanoic  0.1682596 -0.01595516 -0.81991595  0.04591653 -0.46100031
## eicosenoic  0.4384013  0.14034544 -0.41942317  0.08389933  0.60157904
##                   PC6        PC7
## palmitic   -0.4661816 0.10904667
## stearic     0.5067647 0.03928963
## oleic       0.2409267 0.67792957
## linoleic   -0.2371904 0.71467174
## linolenic   0.2916300 0.12220735
## eicosanoic  0.2889776 0.03216008
## eicosenoic -0.4929535 0.01587562&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that PC componennt for each class of oil were added in a &lt;code&gt;prcomp&lt;/code&gt; object.&lt;/p&gt;
&lt;p&gt;And we could plot those component with &lt;code&gt;autoplot&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(pca_res, data = oils, colour = &amp;quot;class&amp;quot;) +
  labs(color = NULL) +theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../2021-02-04-unsupervised-machine-learning-with-tidymodels/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;
We can see that this PCA separates olive oil far away from the other 7 types of oils. It also looks like one of the olive oils is closer to peanunt type of oil in the PCA space .&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pca-in-tidymodels&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;PCA in Tidymodels&lt;/h1&gt;
&lt;p&gt;Modeling is very much like cooking, and in the Tidymodels universe the language is reflects this very well 👩‍🍳. There are three things that we will need to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Writing down a recipe 👩‍🍳&lt;/li&gt;
&lt;li&gt;Preparing that recipe 🍝&lt;/li&gt;
&lt;li&gt;Juicing the recipe 🍵&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;writing-down-a-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Writing down a recipe&lt;/h2&gt;
&lt;p&gt;We write down the recipe by adding series of steps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_rec &amp;lt;- recipe(~., data = oils) %&amp;gt;% # start writing the recipe with all the data
  update_role(class, new_role = &amp;quot;id&amp;quot;) %&amp;gt;% # to keep this column around but not include it in the model
  step_normalize(all_predictors()) %&amp;gt;% # to normalise the data
  step_pca(all_predictors()) # to convert numeric data to principal components&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we see the steps that we need to follow to write the recipe are very similar to the steps followed in base R.
However, this is not all. In fact, if we explore how the recipe looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_rec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##         id          1
##  predictor          7
## 
## Operations:
## 
## Centering and scaling for all_predictors()
## No PCA components were extracted.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the design matrix with id and predictor variables was created. The recipe tells us that the &lt;em&gt;No PCA components were extracted&lt;/em&gt;. This is because a recipe specifies what we want to do, but it doesn’t really do anything to the data yet. We need to extract those components by preparing the recipe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preparing-that-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preparing that recipe&lt;/h2&gt;
&lt;p&gt;We can use the function &lt;code&gt;prep&lt;/code&gt; for preparing to train this data recipe. Prep returns an updated recipe with the estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_prep &amp;lt;- prep(pca_rec)
pca_prep&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Data Recipe
## 
## Inputs:
## 
##       role #variables
##         id          1
##  predictor          7
## 
## Training data contained 96 data points and no missing data.
## 
## Operations:
## 
## Centering and scaling for palmitic, stearic, oleic, linoleic, ... [trained]
## PCA extraction with palmitic, stearic, oleic, linoleic, ... [trained]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the operations we see that the data has been [trained].&lt;/p&gt;
&lt;p&gt;Great! But these are still not the components 🤔. We need to finalise that prepared recipe by &lt;strong&gt;juicing it&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;juicing-the-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Juicing the recipe&lt;/h2&gt;
&lt;p&gt;We need to apply these operation to the data; &lt;code&gt;juice&lt;/code&gt; returns a tibble where all steps have been applied to the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_juiced &amp;lt;- juice(pca_prep)
pca_juiced&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 96 x 6
##    class      PC1    PC2     PC3      PC4      PC5
##    &amp;lt;fct&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 pumpkin -1.22  -0.296 -0.245  -0.158    0.0882 
##  2 pumpkin -1.10  -0.771 -0.198  -0.00964 -0.0901 
##  3 pumpkin -1.08  -1.06  -0.212   0.0154   0.00279
##  4 pumpkin -1.14  -0.266 -0.192  -0.177   -0.137  
##  5 pumpkin -1.25  -0.995 -0.241   0.226   -0.186  
##  6 pumpkin  0.572 -0.500 -0.0821  0.0652   0.286  
##  7 pumpkin -1.13  -0.530 -0.202  -0.0640  -0.0592 
##  8 pumpkin -1.13  -0.530 -0.202  -0.0640  -0.0592 
##  9 pumpkin -1.08  -1.06  -0.212   0.0154   0.00279
## 10 pumpkin -1.14  -0.266 -0.192  -0.177   -0.137  
## # … with 86 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! The processed data is ready to “consumed” by a plot!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_juiced %&amp;gt;%
  ggplot(aes(PC1, PC2, label = class)) +
  geom_point(aes(color = class), alpha = 0.7, size = 2) +
  labs(color = NULL) +theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../2021-02-04-unsupervised-machine-learning-with-tidymodels/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The initial PCA and this one generated with Tidymodels look very similar. Note that autoplot adds some information to the plot such as providing PCs percentage. So what’s the point of using Tidymodels if is a such a long series of steps compared to base R? Well, &lt;a href=&#34;https://www.tidymodels.org/&#34;&gt;Tidymodels&lt;/a&gt; integrates a lot of modular packages which facilitates creating and evaluating different models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;umap-in-tidymodels&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;UMAP in Tidymodels&lt;/h1&gt;
&lt;p&gt;In addition to PCA, we could plot a &lt;a href=&#34;https://umap-learn.readthedocs.io/en/latest/&#34;&gt;UMAP representation&lt;/a&gt;. To do that we would need a new recipe, one that includes a step specify UMAP dimension reduction technique; this step is naturally called &lt;code&gt;step_umap&lt;/code&gt;. Once that we have this recipe, the process is the same. Recipe, prep, juice.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_rec &amp;lt;- recipe(~., data = oils) %&amp;gt;%
  update_role(class, new_role = &amp;quot;id&amp;quot;) %&amp;gt;%
  step_normalize(all_predictors()) %&amp;gt;%
  step_umap(all_predictors()) # this step makes a different recipe &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_prep &amp;lt;- prep(umap_rec)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_juiced &amp;lt;- juice(umap_prep)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;umap_juiced %&amp;gt;%
  ggplot(aes(umap_1, umap_2, label = class)) +
  geom_point(aes(color = class), alpha = 0.7, size = 2) +
  labs(color = NULL)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;featured2.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This model separates the data in the space somewhat differently to PCA. PCA and UMAP are fundamentally different in that PCA is a linear dimensionality reduction algorithm whereas UMAP is non-linear. Moreover, there are few important parameters that can impact how the UMAP representation looks like. This is nicely explained in the README of &lt;code&gt;umapr&lt;/code&gt; package from the &lt;a href=&#34;https://github.com/ropenscilabs/umapr&#34;&gt;ropenscilabs&lt;/a&gt;. You can see additional arguments offered by step_umap with &lt;code&gt;?step_umap&lt;/code&gt;. Also note that we have trained our models with a tiny set of data (we have not done resampling) and we have not evaluated their performance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;The data processing for doing unsupervised machine learning with Tidymodels are very similar to base R. Linear and non-linear dimensionality reduction algorithms separate the data in the reduced space differently.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Word network of Bioconductor packages</title>
      <link>/2021-01-30-network-visualization-of-bioconductor-packages/</link>
      <pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/2021-01-30-network-visualization-of-bioconductor-packages/</guid>
      <description>
&lt;script src=&#34;../2021-01-30-network-visualization-of-bioconductor-packages/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#statistics-of-bioconductor-downloads&#34;&gt;Statistics of Bioconductor downloads&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#full-details-of-bioconductor-packages&#34;&gt;Full details of Bioconductor packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#word-network-of-bioconductor-packages&#34;&gt;Word network of Bioconductor packages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(BiocPkgTools)
library(tidyverse)
library(tidytext)
library(widyr)
library(igraph)
library(ggraph)
library(lubridate)
library(emo)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Bioconductor has a total of 5796 at the present day 2021-01-31. Therefore, navigating across Bioconductor packages can be a daunting experience. Luckily, &lt;a href=&#34;https://seandavi.github.io/BiocPkgTools/&#34;&gt;BiocPkgTools&lt;/a&gt; offers a rich ecosystem of metadata around Bioconductor packages 📜.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistics-of-bioconductor-downloads&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Statistics of Bioconductor downloads&lt;/h1&gt;
&lt;p&gt;We can get a tidy data.frame with download stats for all packages using the function &lt;code&gt;biocDownloadStats&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#  Getting a tidy tibble summarizing monthly download statistics 
bio_download_stats &amp;lt;- biocDownloadStats()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_download_stats %&amp;gt;% 
  head(13)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 x 7
##    Package  Year Month Nb_of_distinct_IPs Nb_of_downloads repo     Date      
##    &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;              &amp;lt;int&amp;gt;           &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;date&amp;gt;    
##  1 ABarray  2021 Jan                   54             114 Software 2021-01-01
##  2 ABarray  2021 Feb                    0               0 Software 2021-02-01
##  3 ABarray  2021 Mar                    0               0 Software 2021-03-01
##  4 ABarray  2021 Apr                    0               0 Software 2021-04-01
##  5 ABarray  2021 May                    0               0 Software 2021-05-01
##  6 ABarray  2021 Jun                    0               0 Software 2021-06-01
##  7 ABarray  2021 Jul                    0               0 Software 2021-07-01
##  8 ABarray  2021 Aug                    0               0 Software 2021-08-01
##  9 ABarray  2021 Sep                    0               0 Software 2021-09-01
## 10 ABarray  2021 Oct                    0               0 Software 2021-10-01
## 11 ABarray  2021 Nov                    0               0 Software 2021-11-01
## 12 ABarray  2021 Dec                    0               0 Software 2021-12-01
## 13 ABarray  2021 all                   54             114 Software NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we see observations for all the months of the year are generated once that the year starts (download values for events in the future are filled up with &lt;em&gt;0&lt;/em&gt;). Also note that there is a summary statistic for month called &lt;code&gt;all&lt;/code&gt; embedded inside the tibble, and the &lt;code&gt;Date&lt;/code&gt; value for that observation is NA (this would makes group by date very convenient).&lt;/p&gt;
&lt;p&gt;This tibble contains information about packages that expands from 2009 to 2021. There are 3 categories of packages, with the total number of package per category as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bio_download_stats %&amp;gt;% 
  distinct(Package, repo) %&amp;gt;%
  count(repo) %&amp;gt;%
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;repo&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;AnnotationData&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2659&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ExperimentData&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;821&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Software&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2316&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;full-details-of-bioconductor-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Full details of Bioconductor packages&lt;/h1&gt;
&lt;p&gt;The complete information for the packages as described in the &lt;code&gt;DESCRIPTION&lt;/code&gt; file can be obtained with &lt;code&gt;biocPkgList&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bpi = biocPkgList()
colnames(bpi)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;Package&amp;quot;               &amp;quot;Version&amp;quot;               &amp;quot;Depends&amp;quot;              
##  [4] &amp;quot;Suggests&amp;quot;              &amp;quot;License&amp;quot;               &amp;quot;MD5sum&amp;quot;               
##  [7] &amp;quot;NeedsCompilation&amp;quot;      &amp;quot;Title&amp;quot;                 &amp;quot;Description&amp;quot;          
## [10] &amp;quot;biocViews&amp;quot;             &amp;quot;Author&amp;quot;                &amp;quot;Maintainer&amp;quot;           
## [13] &amp;quot;git_url&amp;quot;               &amp;quot;git_branch&amp;quot;            &amp;quot;git_last_commit&amp;quot;      
## [16] &amp;quot;git_last_commit_date&amp;quot;  &amp;quot;Date/Publication&amp;quot;      &amp;quot;source.ver&amp;quot;           
## [19] &amp;quot;win.binary.ver&amp;quot;        &amp;quot;mac.binary.ver&amp;quot;        &amp;quot;vignettes&amp;quot;            
## [22] &amp;quot;vignetteTitles&amp;quot;        &amp;quot;hasREADME&amp;quot;             &amp;quot;hasNEWS&amp;quot;              
## [25] &amp;quot;hasINSTALL&amp;quot;            &amp;quot;hasLICENSE&amp;quot;            &amp;quot;Rfiles&amp;quot;               
## [28] &amp;quot;dependencyCount&amp;quot;       &amp;quot;Imports&amp;quot;               &amp;quot;Enhances&amp;quot;             
## [31] &amp;quot;dependsOnMe&amp;quot;           &amp;quot;VignetteBuilder&amp;quot;       &amp;quot;suggestsMe&amp;quot;           
## [34] &amp;quot;LinkingTo&amp;quot;             &amp;quot;Archs&amp;quot;                 &amp;quot;URL&amp;quot;                  
## [37] &amp;quot;SystemRequirements&amp;quot;    &amp;quot;BugReports&amp;quot;            &amp;quot;importsMe&amp;quot;            
## [40] &amp;quot;Video&amp;quot;                 &amp;quot;linksToMe&amp;quot;             &amp;quot;OS_type&amp;quot;              
## [43] &amp;quot;PackageStatus&amp;quot;         &amp;quot;License_restricts_use&amp;quot; &amp;quot;License_is_FOSS&amp;quot;      
## [46] &amp;quot;organism&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is lots of information in here. We could use this metadata information to understand the connections between packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;word-network-of-bioconductor-packages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Word network of Bioconductor packages&lt;/h1&gt;
&lt;p&gt;The most informative variables about the packages are &lt;code&gt;Title&lt;/code&gt; and &lt;code&gt;Description&lt;/code&gt; so we can explore the connections between packages doing some &lt;strong&gt;text mining&lt;/strong&gt; using a &lt;a href=&#34;https://www.tidytextmining.com/index.html&#34;&gt;Tidytext&lt;/a&gt; approach.&lt;/p&gt;
&lt;p&gt;To prepare our dataset we need to initially tokenize the text. The Wikipedia definition for &lt;a href=&#34;https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization&#34;&gt;tokenization&lt;/a&gt; on lexical analysis is as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Tokenization is the process of demarcating and possibly classifying sections of a string of input characters&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;em&gt;sections&lt;/em&gt; can be words, sentence, ngram or chapter (for example if analysis a book). In this case we are gonna break down package Titles or Description into words using the function &lt;code&gt;unnest_tokens&lt;/code&gt;.
In addition, we can remove &lt;a href=&#34;https://en.wikipedia.org/wiki/Stop_word&#34;&gt;stop words&lt;/a&gt; (included in the Tidytext dataset).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bpi_title &amp;lt;- bpi %&amp;gt;% 
  dplyr::select(Package, Title) %&amp;gt;%
  unnest_tokens(word, Title) %&amp;gt;% 
  anti_join(stop_words)

bpi_description &amp;lt;- bpi %&amp;gt;%
  dplyr::select(Package, Description) %&amp;gt;%
  unnest_tokens(word, Description) %&amp;gt;%
  anti_join(stop_words)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the number of words from Title is 11932 and the number
of words from Description is 59370, so package Descriptions
contain on average 5 times the words of package Titles.&lt;/p&gt;
&lt;p&gt;We can have a look at how the tokenised titles for each package look like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bpi_title&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 11,932 x 2
##    Package word      
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;     
##  1 a4      automated 
##  2 a4      affymetrix
##  3 a4      array     
##  4 a4      analysis  
##  5 a4      umbrella  
##  6 a4      package   
##  7 a4Base  automated 
##  8 a4Base  affymetrix
##  9 a4Base  array     
## 10 a4Base  analysis  
## # … with 11,922 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Them, we can use &lt;code&gt;pairwise_count&lt;/code&gt; from the &lt;a href=&#34;https://cran.r-project.org/web/packages/widyr/vignettes/intro.html&#34;&gt;widyr&lt;/a&gt; package to count how many times each pair of words occurs together in the package Title. This function works as a mutate in that it takes the variables to compare and returns a tibble with the pairwise columns and an extra column called &lt;code&gt;n&lt;/code&gt; containing the number of words co-occurrences. I think this function is very sweet 🍯!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bpi_titlepairs &amp;lt;- bpi_title %&amp;gt;%
pairwise_count(Package, word, sort = TRUE, upper = FALSE)

bpi_desciptionpairs &amp;lt;- bpi_description %&amp;gt;%
pairwise_count(Package, word, sort = TRUE, upper = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This data is ready for visualization of network of co-occurring words in package Titles. We can use the &lt;code&gt;ggraph&lt;/code&gt; package for visualizing this network. We are going to represent just the top co-occurring words, or otherwise we get a very populated network which is impossible to read.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
bpi_titlepairs %&amp;gt;%
  filter(n &amp;gt;= 6) %&amp;gt;%
  graph_from_data_frame() %&amp;gt;%
  ggraph(layout = &amp;quot;fr&amp;quot;) +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = &amp;quot;purple&amp;quot;) +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, &amp;quot;lines&amp;quot;)) +
  theme_void()+
  theme(legend.position=&amp;quot;none&amp;quot;)+
  labs(title = &amp;quot;  Number of word co-ocurrences in packages titles&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-10&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-01-30-network-visualization-of-bioconductor-packages/index_files/figure-html/unnamed-chunk-10-1.png&#34; alt=&#34;Word network in Bioconductor packages Titles&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Word network in Bioconductor packages Titles
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We see some clear and logical clustering of packages in this network.For example, DESEq and DESeq2 packages cluster together, as one would expect since they DESeq2 is the successor of DESeq. There are other obvious connections such as MSstatsTMTPTM and MSstatsTMTP since the former has added functionality to analyse PTMs on TMT shotgun mass spectrometry-based proteomic experiments.
There is a big cluster on the bottom left corner with packages to analyse RNASeq and single cell RNASeq.&lt;/p&gt;
&lt;p&gt;What about the network build from words of the Description?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
bpi_desciptionpairs %&amp;gt;%
  filter(n &amp;gt;= 15) %&amp;gt;%
  graph_from_data_frame() %&amp;gt;%
  ggraph(layout = &amp;quot;fr&amp;quot;) +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = &amp;quot;orange&amp;quot;) +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, &amp;quot;lines&amp;quot;)) +
  theme_void()+
  theme(legend.position=&amp;quot;none&amp;quot;)+
  labs(title = &amp;quot;Number of word co-ocurrences in packages Description&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-01-30-network-visualization-of-bioconductor-packages/index_files/figure-html/unnamed-chunk-11-1.png&#34; alt=&#34;Word network in Bioconductor packages Description&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Word network in Bioconductor packages Description
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We see more connections here, and some of the relationships are still obvious (e.g HiCcompare and multiHiCcompare, anota and anota2seq, AnnotationHub and ExperimentHub). This network is richer, and one would have to dive a bit deeper to get a better sense of this network.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;Text mining of Bioconductor packages metadata is a straight forward visual way to understand the relationships between packages. One could go beyond this and for example finding words that are
especially important across package Descriptions by calculating &lt;a href=&#34;https://www.tidytextmining.com/tfidf.html#tfidf&#34;&gt;tf-idf statistic&lt;/a&gt;. One could also set up a GitHub Action executed as a CRON job to get updates periodically. This could turn into a challenge for &lt;a href=&#34;https://kevinrue.github.io/BiocChallenges/index.html&#34;&gt;BiocChallenges&lt;/a&gt;. This post was inspired by &lt;a href=&#34;https://www.tidytextmining.com/nasa.html&#34;&gt;Chapter 8&lt;/a&gt; of the Tidytext book and &lt;a href=&#34;https://kevinrue.github.io/post/biocpkgtools/&#34;&gt;BiocRoulette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Joining the RStudio Tidyverse Instructor community</title>
      <link>/2021-01-20-rstudio-instructor-certification-tidyverse/</link>
      <pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/2021-01-20-rstudio-instructor-certification-tidyverse/</guid>
      <description>
&lt;script src=&#34;../2021-01-20-rstudio-instructor-certification-tidyverse/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fundations-of-my-motivation&#34;&gt;Fundations of my motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#why-i-decided-to-become-an-rstudio-tidyverse&#34;&gt;Why I decided to become an RStudio Tidyverse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#during-the-training&#34;&gt;During the training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#between-training-and-exam&#34;&gt;Between training and exam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#during-the-teaching-exam&#34;&gt;During the teaching exam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#during-the-tidyverse-exam&#34;&gt;During the Tidyverse exam&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#voilà&#34;&gt;Voilà!&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;fundations-of-my-motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fundations of my motivation&lt;/h1&gt;
&lt;p&gt;Early 2020, in a pre-pandemic world, I attended my first &lt;a href=&#34;https://mine-cetinkaya-rundel.github.io/tidy-up-ds/2020-02-london/tidy-up.html#1&#34;&gt;Rladies meetup in London&lt;/a&gt; by
&lt;a href=&#34;https://education.rstudio.com/author/mine/&#34;&gt;Mine Cetinkaya-Rundel&lt;/a&gt;. Part of the reasons I decided to attended was because my husband was going to be away for the foreseeable months, so I decided to give myself the chance to connect with other Rladies. It was only few months before that meetup that I started using the Tidyverse, although I used R for many years in the past. It was a fantastic talk where I met some very cool Rladies and I got super impressed by Mine’s teaching abilities.&lt;/p&gt;
&lt;p&gt;During the past year, and because of continous lockdowns and WFH, I really dived into the Tidyverse: I joint LatinR talks, participated in Rladies Barcelona Florence Nightingale content, I watched Tidytuesday screencasts, opened a blog with Hugo, Rblogdown and Netlify (thanks to the amazing resources shared by &lt;a href=&#34;https://education.rstudio.com/trainers/people/hill+alison/&#34;&gt;Alison Hill&lt;/a&gt;). I could easily say that the Tidyverse mission 🛰 keep my mental health afloat during such difficult times.&lt;/p&gt;
&lt;p&gt;Moreover, during the summer of 2020 I got some formal teaching training from university of London where I learn concepts like Blooms taxonomy and I started to give by-weekly workshops for biology researcher co-workers of mine where I introduce them to ggplot2 and statistical concepts such as correlation.&lt;/p&gt;
&lt;p&gt;All these things had a direct impact on my research career; for instance I published my
first R package and gave my first EuroBioconductor &lt;a href=&#34;https://docs.google.com/presentation/d/16WN2ldl0r4f5Iv1pR5DAdSIC29H5JcM1Dr0RrYH9qx4/edit#slide=id.p&#34;&gt;short talk&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-i-decided-to-become-an-rstudio-tidyverse&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why I decided to become an RStudio Tidyverse&lt;/h1&gt;
&lt;p&gt;I decided to get certificated for several reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Getting additional formal training on how to teach. I felt I and my audience could benefit more the better trained for training I would be.&lt;/li&gt;
&lt;li&gt;Pushing myself to really know inside out the Tidyverse. Getting the training forced me to read the R4DS book and doing the exercises thoroughly. I used additional fantastic resources such as &lt;a href=&#34;https://datasciencebox.org/&#34;&gt;https://datasciencebox.org/&lt;/a&gt;. Later, I learn that there are book clubs for some of the most popular R books, something that I somewhat missed, but I am very excited to join in 2021 the tidymodels book club organised by &lt;a href=&#34;https://twitter.com/JonTheGeek&#34;&gt;Jon Harmon&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The RStudio Instructor training journey has properly equipped me to do and teach data science! This process pushed me out of my comfort zone for which I will be forever grateful.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;during-the-training&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;During the training&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Instructor training covering modern teaching methods&lt;/strong&gt;. Materials for this training are &lt;a href=&#34;https://drive.google.com/drive/folders/13ohFt3D0EJ5PDbMaWTxnHH-hwA7G0IvY&#34;&gt;available here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This was a two half days course where &lt;a href=&#34;https://third-bit.com/&#34;&gt;Greg Wilson&lt;/a&gt; lead such inspiring modules. What I like the most was feeling part of the learning community, the interactivity of the modules and genuinely learning about evidence-based effective teaching methods.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Be nice, the rest is on the details &lt;strong&gt;–Greg&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;between-training-and-exam&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Between training and exam&lt;/h1&gt;
&lt;div id=&#34;materials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Materials&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Books&lt;/strong&gt;.
Reading the book &lt;a href=&#34;https://teachtogether.tech/&#34;&gt;Teaching Tech Together&lt;/a&gt; written by Greg. The book is currently undergoing translation into Spanish by a team of volunteers and I am honored to be among them thanks to &lt;a href=&#34;https://education.rstudio.com/trainers/people/bellini_saibene+yanina/&#34;&gt;Yanina Bellini Saibene&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Guides&lt;/strong&gt;.
Going through &lt;a href=&#34;https://github.com/rstudio-education/r4ds-instructors&#34;&gt;R4DS instructor’s guide&lt;/a&gt; which has learning objectives and key points for each chapter of R4DS book.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Concept maps&lt;/strong&gt;.
Building concept maps to design lessons taking into account cognitive load is a big part of the training. When preparing for the two exams I found &lt;a href=&#34;https://github.com/rstudio/concept-maps/tree/master/es&#34;&gt;this resource&lt;/a&gt; extremely useful to check I was building an accurate mental model of the topics covered in R4DS. I built myself quite a lot of mental models that you can find here&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Greg also asked us to send an email with what we remember and so I did this concept map (there is a lot of information buried beneath my bad hand writing)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;RStudioInstructor_1month_MariaDermit.jpg&#34; /&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Watching and reading other people’s experiences&lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SJmfXYd0hOU&amp;amp;feature=emb_logo&#34;&gt;Process to become a certified Rstudio instructor&lt;/a&gt; a video where some Mi-useRs &lt;a href=&#34;https://medium.com/@doritolay/introducing-mir-a-community-for-underrepresented-users-of-r-7560def7d861&#34;&gt;(minority R users)&lt;/a&gt; talk about their experiences and motivations to become a certified Rstudio instructor.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://yabellini.netlify.app/post/rstudiocertification/&#34;&gt;Obtaining RStudio certification. A shared path&lt;/a&gt; a post by Yanina on her journey to obtain RStudio certification.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://bcullen.rbind.io/post/2020-09-03-reflections-on-rstudio-instructor-training/&#34;&gt;Reflections&lt;/a&gt; by &lt;a href=&#34;https://education.rstudio.com/trainers/people/cullen+brendan/&#34;&gt;Brendan Cullen’s&lt;/a&gt; on RStudio Instructor Training .&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://silvia.rbind.io/2020-10-07-rstudio-instructor-certification-tidyverse/&#34;&gt;Overview&lt;/a&gt; by &lt;a href=&#34;https://education.rstudio.com/trainers/people/canelon+silvia/&#34;&gt;Silvia Canelon&lt;/a&gt;
of the RStudio Instructor certification process. Check her post since she has an amazing collection of resources to support anyone on their certification journey.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Active learning &lt;/strong&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;With some of the materials I use during my learning, I created some exercises using the &lt;a href=&#34;https://rstudio.github.io/learnr/&#34;&gt;learnr&lt;/a&gt; package, which is such a powerful and versatile way to create interactive exercises.
&lt;a href=&#34;https://clif.shinyapps.io/01-Replacement-es/&#34;&gt;This&lt;/a&gt; is my humble contribution to fading exercises with &lt;a href=&#34;https://github.com/allisonhorst/palmerpenguins&#34;&gt;Penguins&lt;/a&gt; and I hope to extend these materials in &lt;a href=&#34;https://user2021.r-project.org/&#34;&gt;user!2021&lt;/a&gt; later this year&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I also gave a mocked presentation to &lt;a href=&#34;https://education.rstudio.com/trainers/people/quiroga+riva/&#34;&gt;Riva Quiroga&lt;/a&gt; and &lt;a href=&#34;https://education.rstudio.com/trainers/people/bellini_saibene+yanina/&#34;&gt;Yanina Bellini&lt;/a&gt; over Zoom and received extremely useful feedback. Gracias chicas!!&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;during-the-teaching-exam&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;During the teaching exam&lt;/h1&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A 90-minute &lt;strong&gt;teaching exam&lt;/strong&gt; based on content from the instructor training. This includes a 15-minute demonstration lesson and a written component. Mine was on &lt;a href=&#34;https://r4ds.had.co.nz/workflow-projects.html&#34;&gt;RStudio projects&lt;/a&gt; and my learner persona was &lt;a href=&#34;https://docs.google.com/presentation/d/11CN4Ox8k-zeFEoXJYkbWTNN0V94pyS-T8oHMmBG34xU/edit#slide=id.ga7003f49f1_0_78&#34;&gt;Lovorka&lt;/a&gt;. You can see my contribution &lt;a href=&#34;https://docs.google.com/presentation/d/124emaF1lpe4PwVC2zy2bxN1-ckGynlPBkFxVRh7a0r8/edit&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;during-the-tidyverse-exam&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;During the Tidyverse exam&lt;/h1&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;A 90-minute &lt;strong&gt;Tidyverse exam&lt;/strong&gt; to test your knowledge of the subject matter in &lt;a href=&#34;https://r4ds.had.co.nz/&#34;&gt;R for Data Science&lt;/a&gt; (make sure to check the &lt;a href=&#34;https://education.rstudio.com/trainers/#faq&#34;&gt;program FAQs&lt;/a&gt; for information about the Shiny exam). I hope to share my exercise soon.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;voilà&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Voilà!&lt;/h1&gt;
&lt;p&gt;Joining this community is a dream… that may become true for you too? 🍎&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Transfer learning for spatial proteomics</title>
      <link>/2021-01-20-transfer-learning-for-spatial-proteomics/</link>
      <pubDate>Wed, 20 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/2021-01-20-transfer-learning-for-spatial-proteomics/</guid>
      <description>
&lt;script src=&#34;../2021-01-20-transfer-learning-for-spatial-proteomics/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#getting-the-data&#34;&gt;Getting the data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#quetting-auxiliary-annotation-biomart-query&#34;&gt;Quetting auxiliary annotation (Biomart Query)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#nearest-neighbour-transfer-learning&#34;&gt;Nearest neighbour transfer learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusions&#34;&gt;Conclusions&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# loading libraries
# clearing environment bc https://support.bioconductor.org/p/p132709/
rm(list = ls())
library(pRoloc)
library(pRolocdata)
library(BiocStyle)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Within the cell, the localization of a given protein is determined by its biological
function. Subcellular proteomics is the method to study protein sub-cellular
localization in a systematic manner. There are two complementary ways to analysis localized proteins:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;On one hand biochemical sub-cellular fractionation experiments allow empirical quantification of protein across sub-cellular and sub-organellar compartments. Proteins are allocated to a given subcellular niche
if the detected intensity is higher than a threshold. We can say that this type of data has high signal-to-noise ratio, but is available in limited amounts (&lt;em&gt;primary&lt;/em&gt; data).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;On the other hand, databases such as &lt;a href=&#34;http://geneontology.org/&#34;&gt;GO&lt;/a&gt; contain large amount of information about sub-cellular proteins localisation, but this is information is blended for a many biological systems. We can say that this type of data has high low signal-to-noise, but is available in large amounts (&lt;em&gt;auxiliary&lt;/em&gt; data).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So we want to know &lt;em&gt;how to optimally combine&lt;/em&gt; primary and auxiliary data.💹&lt;/p&gt;
&lt;p&gt;To do so, we need to weight both types of data. If we imagine a multivariate
distribution (like a &lt;a href=&#34;https://github.com/TommyJones/tidylda&#34;&gt;Dirichlet distribution&lt;/a&gt;) were all the components take values between (0,1) and all values sum up to one, we can imagine that a weight of 1 indicates that the final annotation rely exclusively on the experimental data and ignore completely the auxiliary data and a weight of 0 represents the opposite situation, where the primary data is ignored and only the auxiliary data is considered.&lt;/p&gt;
&lt;p&gt;We could use a &lt;em&gt;transfer learning&lt;/em&gt; algorithm to efficiently complement the primary data with auxiliary data without compromising the integrity of the former. This is implemented in the &lt;code&gt;pRoloc&lt;/code&gt; package and it was published by &lt;a href=&#34;https://lgatto.github.io/pRoloc/reference/knntlClassification.html&#34;&gt;Breckels et al&lt;/a&gt; and expanded by
&lt;a href=&#34;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006516&#34;&gt;Crook et al&lt;/a&gt; using a Bayesian approach. In this post I will step-by-step walk through KNN transfer learning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting the data&lt;/h1&gt;
&lt;p&gt;We start with &lt;code&gt;HEK293T2011&lt;/code&gt; proteomics data available in the &lt;em&gt;&lt;a href=&#34;https://bioconductor.org/packages/3.10/pRolocdata&#34;&gt;pRolocdata&lt;/a&gt;&lt;/em&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(HEK293T2011)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The class of (HEK293T2011) isMSnSet instance, an efficient and established to &lt;a href=&#34;http://www.bioconductor.org/packages/release/bioc/html/MSnbase.html&#34;&gt;store and handle MS data and metainformation efficiently&lt;/a&gt;. I am not going to discuss much about this class of objects since the field is moving towards other types of data storage such as &lt;a href=&#34;https://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html&#34;&gt;SummarizedExperiment objects&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can also get an overview experimental data and query how many proteins across how many conditions were quantified.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(exprs(HEK293T2011),2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             X113      X114       X115      X116      X117       X118       X119
## O00767 0.1360547 0.1495961 0.10623931 0.1465050 0.2773137 0.14294025 0.03796970
## P51648 0.1914456 0.2052463 0.05661169 0.1651138 0.2366302 0.09964387 0.01803788
##               X121
## O00767 0.003381233
## P51648 0.027270640&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dim(exprs(HEK293T2011))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1371    8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is important to know is that 1371 proteins were quantified across eight iTRAQ 8-plex labelled fractions (
one could know a bit more about the experiment with &lt;code&gt;?HEK293T2011&lt;/code&gt;)&lt;/p&gt;
&lt;p&gt;Next thing we can do is see how well these organelles have been resolved in the experiment using a PCA plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot2D(HEK293T2011)
addLegend(HEK293T2011, where = &amp;quot;topright&amp;quot;, cex = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;../2021-01-20-transfer-learning-for-spatial-proteomics/index_files/figure-html/unnamed-chunk-4-1.png&#34; alt=&#34;PCA plot of `HEK293T2011 subcellular proteomics dataset`&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: PCA plot of &lt;code&gt;HEK293T2011 subcellular proteomics dataset&lt;/code&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We see that some organelles such as cytosol and cytosol/nucleus are well resolved - and so they will get a high weigh- while others such as the Golgi or the ER are less so - so they will get a low weight.
There are some proteins that do not get annotation because the resolution of the experiment did not allow so.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quetting-auxiliary-annotation-biomart-query&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quetting auxiliary annotation (Biomart Query)&lt;/h1&gt;
&lt;p&gt;Next thing we can do is get auxiliary data. We can do so by querying &lt;em&gt;&lt;a href=&#34;https://bioconductor.org/packages/3.10/biomaRt&#34;&gt;biomaRt&lt;/a&gt;&lt;/em&gt; and storing the annotation as an &lt;code&gt;AnnotationParams&lt;/code&gt; object. Again, this is part of the &lt;em&gt;&lt;a href=&#34;https://bioconductor.org/packages/3.10/pRoloc&#34;&gt;pRoloc&lt;/a&gt;&lt;/em&gt; package, and it has been created for efficient data handling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ap &amp;lt;- setAnnotationParams(inputs =
                              c(&amp;quot;Human genes&amp;quot;,
                                &amp;quot;UniProtKB/Swiss-Prot ID&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Connecting to Biomart...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can access this instance with&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getAnnotationParams()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Object of class &amp;quot;AnnotationParams&amp;quot;
##  Using the &amp;#39;ENSEMBL_MART_ENSEMBL&amp;#39; BioMart database
##  Using the &amp;#39;hsapiens_gene_ensembl&amp;#39; dataset
##  Using &amp;#39;uniprotswissprot&amp;#39; as filter
##  Created on Tue Jan 26 15:47:22 2021&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can annotate our innitial &lt;code&gt;HEK293T2011&lt;/code&gt; data by creating a new &lt;code&gt;MSnSet&lt;/code&gt; instance populated with a GO term as a binary matrix (so the auxiliary data with information about 889 cellular compartment GO terms has been added).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HEK293T2011goset  &amp;lt;- makeGoSet(HEK293T2011)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nearest-neighbour-transfer-learning&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Nearest neighbour transfer learning&lt;/h1&gt;
&lt;div id=&#34;deciding-the-weight&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Deciding the weight&lt;/h2&gt;
&lt;p&gt;We could define more or less weight values between 0 and 1 depending on how granular we want to be with
our search (more weight will give finer-grained integration).For example for 3 classes, 3 weights will generate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gtools::permutations(length(seq(0, 1, 0.5)), 3, seq(0, 1, 0.5), repeats.allowed = TRUE) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       [,1] [,2] [,3]
##  [1,]  0.0  0.0  0.0
##  [2,]  0.0  0.0  0.5
##  [3,]  0.0  0.0  1.0
##  [4,]  0.0  0.5  0.0
##  [5,]  0.0  0.5  0.5
##  [6,]  0.0  0.5  1.0
##  [7,]  0.0  1.0  0.0
##  [8,]  0.0  1.0  0.5
##  [9,]  0.0  1.0  1.0
## [10,]  0.5  0.0  0.0
## [11,]  0.5  0.0  0.5
## [12,]  0.5  0.0  1.0
## [13,]  0.5  0.5  0.0
## [14,]  0.5  0.5  0.5
## [15,]  0.5  0.5  1.0
## [16,]  0.5  1.0  0.0
## [17,]  0.5  1.0  0.5
## [18,]  0.5  1.0  1.0
## [19,]  1.0  0.0  0.0
## [20,]  1.0  0.0  0.5
## [21,]  1.0  0.0  1.0
## [22,]  1.0  0.5  0.0
## [23,]  1.0  0.5  0.5
## [24,]  1.0  0.5  1.0
## [25,]  1.0  1.0  0.0
## [26,]  1.0  1.0  0.5
## [27,]  1.0  1.0  1.0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As we sayed before, HEK293T2011goset experiment has 10 subcellular compartments, and so the total combinations for 10 classes, 4 weights will be:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;th &amp;lt;- gtools::permutations(length(seq(0, 1, length.out = 4)), 10, seq(0, 1, length.out = 4), repeats.allowed = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Total combination of weights for HEK293T2011goset experiment will be 1048576.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://bioconductor.org/packages/3.10/pRoloc&#34;&gt;pRoloc&lt;/a&gt;&lt;/em&gt; package comes with a convenient function &lt;code&gt;thetas&lt;/code&gt; to produce such a weight matrix (because we need a theta for each of the training feature).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## marker classes for HEK293T2011
m &amp;lt;- unique(fData(HEK293T2011)$markers.tl)
m &amp;lt;- m[m != &amp;quot;unknown&amp;quot;]
th &amp;lt;- thetas(length(m), length.out=4)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;optimizing-weigth&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimizing weigth&lt;/h2&gt;
&lt;p&gt;We can do a grid search to determine which is the best &lt;code&gt;th&lt;/code&gt;, with the &lt;code&gt;knntlOptimisation&lt;/code&gt; function of the &lt;em&gt;&lt;a href=&#34;https://bioconductor.org/packages/3.10/pRoloc&#34;&gt;pRoloc&lt;/a&gt;&lt;/em&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;topt &amp;lt;- knntlOptimisation(HEK293T2011, HEK293T2011goset,
                          th = th,
                          k = c(3, 3),
                          fcol = &amp;quot;markers.tl&amp;quot;,
                          times = 50, 
                          method = &amp;quot;Breckels&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the sake of time, we can reduce our initial data, as it will take a long time to do this grid search (even if &lt;code&gt;knntlOptimisation&lt;/code&gt; uses parallelisation by default).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2021)
i &amp;lt;- sample(nrow(th), 50)
topt &amp;lt;- knntlOptimisation(HEK293T2011, HEK293T2011goset,
                          th = th[i, ],
                          k = c(3, 3),
                          fcol = &amp;quot;markers.tl&amp;quot;,
                          times = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The optimisation is performed on the labelled marker examples only. &lt;code&gt;topt&lt;/code&gt; result stores all the result from the optimisation step, and in particular the observed theta weights, which can be directly plotted as shown on the bubble plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(topt)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://raw.githubusercontent.com/lgatto/pRoloc/master/vignettes/Figures/bubble-andy.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Result stocores for the optimisation step. Note that this figure is the result using extensive optimisation on the whole HEK293T2011 dataset and auxiliary HEK293T2011goset dataset, not only with only a random subset of 50 candidate weights.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We see that the cytosol and cytosol/nucleus and ER are predominantly scored with high heights, consistent with high reliability of the primary data. Golgi, PM and the 40S ribosomal clusters are scored with smaller scores, indicating a substantial benefit of the auxiliary data.&lt;/p&gt;
&lt;p&gt;The best grid search parameters can be accessed with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;getParams(topt)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the data &lt;code&gt;HEK293T2011&lt;/code&gt; &lt;em&gt;gets annotated&lt;/em&gt; with the best parameters at the knntlOptimisation
step. We can get the best weights with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bw &amp;lt;- experimentData(HEK293T2011)@other$knntl$thetas&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performing-transfer-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performing transfer learning&lt;/h2&gt;
&lt;p&gt;To apply the best weights and learn from the auxiliary data to classify the unlabelled proteins into sub-cellular niches (present in &lt;code&gt;markers.tl&lt;/code&gt; column), we can pass the primary and auxiliary data sets, best weights, best k’s and the metadata feature data taht contains the markers definitions to the &lt;code&gt;knntlClassification&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HEK293T2011 &amp;lt;- knntlClassification(HEK293T2011, HEK293T2011goset,
                                bestTheta = bw,
                                k = c(3, 3),
                                fcol = &amp;quot;markers.tl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this step, annotation predictors scores and parameters get added into the MSnSet data. We can access the predicted localization conveniently using the &lt;code&gt;getPredictions&lt;/code&gt; assessor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;HEK293T2011 &amp;lt;- getPredictions(HEK293T2011, fcol = &amp;quot;knntl&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-the-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting the results&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# These functions allow to get/set the colours/points to plot organelle features 
setStockcol(paste0(getStockcol(), &amp;quot;80&amp;quot;))
#this defines the point size
ptsze &amp;lt;- exp(fData(HEK293T2011)$knntl.scores) - 1
plot2D(HEK293T2011, fcol = &amp;quot;knntl&amp;quot;, cex = ptsze)
setStockcol(NULL)
addLegend(HEK293T2011, where = &amp;quot;topright&amp;quot;,
          fcol = &amp;quot;markers.tl&amp;quot;,
          bty = &amp;quot;n&amp;quot;, cex = .7)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;TL_PCA.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;PCA plot of &lt;code&gt;HEK293T2011&lt;/code&gt; after transfer learning classification. The size of the points is proportional to the classification scores.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions&lt;/h1&gt;
&lt;p&gt;A &lt;em&gt;weighted k-nearest neighbour transfer learning&lt;/em&gt; algorithm can be very useful to predict of protein
sub-cellular localisation using quantitative proteomics data as primary data source and Gene Ontology-derived binary data as auxiliary data source.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
